{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Spike-Timing-Dependent Plasticity (STDP)\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- Understand the biological basis of Spike-Timing-Dependent Plasticity (STDP).\n",
    "- Implement the STDP learning rule in Python.\n",
    "- Simulate a Leaky Integrate-and-Fire (LIF) neuron with plastic synapses.\n",
    "- Demonstrate how STDP enables a neuron to learn repeated patterns in input spike trains.\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Basic understanding of the Leaky Integrate-and-Fire (LIF) neuron model.\n",
    "- Familiarity with synaptic transmission.\n",
    "- Basic Python and NumPy skills.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous tutorials, we explored how neurons integrate inputs and generate spikes, and how synapses transmit signals. However, the strength of synapses is not fixed; it changes based on the activity of the pre- and post-synaptic neurons. This **synaptic plasticity** is the cellular basis of learning and memory.\n",
    "\n",
    "**Hebbian learning** is often summarized as \"Cells that fire together, wire together.\" **Spike-Timing-Dependent Plasticity (STDP)** is a biologically detailed form of Hebbian learning that depends on the precise timing of spikes:\n",
    "\n",
    "- If the pre-synaptic neuron fires _before_ the post-synaptic neuron (causal relationship), the synapse is strengthened (**Long-Term Potentiation, LTP**).\n",
    "- If the pre-synaptic neuron fires _after_ the post-synaptic neuron (acausal relationship), the synapse is weakened (**Long-Term Depression, LTD**).\n",
    "\n",
    "In this tutorial, we will implement STDP and show how it allows a neuron to detect repeating patterns in a noisy input stream.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from neuroai import plotting\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. The STDP Learning Rule\n",
    "\n",
    "The change in synaptic weight $\\Delta w$ depends on the relative timing $\\Delta t = t_{post} - t_{pre}$ between the post-synaptic spike and the pre-synaptic spike.\n",
    "\n",
    "The standard STDP function is defined as:\n",
    "\n",
    "$$\n",
    "\\Delta w(\\Delta t) = \\begin{cases}\n",
    "A_+ e^{-\\Delta t/\\tau_+} & \\text{if } \\Delta t > 0 \\quad (\\text{Pre before Post} \\rightarrow \\text{LTP}) \\\\\n",
    "-A_- e^{\\Delta t/\\tau_-} & \\text{if } \\Delta t < 0 \\quad (\\text{Post before Pre} \\rightarrow \\text{LTD})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A_+, A_-$ are the maximum amplitudes of potentiation and depression.\n",
    "- $\\tau_+, \\tau_-$ are the time constants of the STDP window.\n",
    "\n",
    "Let's visualize this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdp_window(delta_t, A_plus=0.01, A_minus=0.01, tau_plus=20, tau_minus=20):\n",
    "    \"\"\"\n",
    "    Calculate the synaptic weight change based on the time difference.\n",
    "\n",
    "    Args:\n",
    "        delta_t (np.array): t_post - t_pre (ms)\n",
    "        A_plus (float): Max potentiation\n",
    "        A_minus (float): Max depression\n",
    "        tau_plus (float): Time constant for LTP (ms)\n",
    "        tau_minus (float): Time constant for LTD (ms)\n",
    "\n",
    "    Returns:\n",
    "        np.array: Weight change delta_w\n",
    "\n",
    "    Examples:\n",
    "        >>> delta_t = np.array([10, -10, 0])\n",
    "        >>> dw = stdp_window(delta_t, A_plus=1.0, A_minus=1.0, tau_plus=10.0, tau_minus=10.0)\n",
    "        >>> np.round(dw, 4)\n",
    "        array([ 0.3679, -0.3679,  0.    ])\n",
    "    \"\"\"\n",
    "    dw = np.zeros_like(delta_t)\n",
    "\n",
    "    # LTP: Pre before Post (delta_t > 0)\n",
    "    mask_ltp = delta_t > 0\n",
    "    dw[mask_ltp] = A_plus * np.exp(-delta_t[mask_ltp] / tau_plus)\n",
    "\n",
    "    # LTD: Post before Pre (delta_t < 0)\n",
    "    mask_ltd = delta_t < 0\n",
    "    dw[mask_ltd] = -A_minus * np.exp(delta_t[mask_ltd] / tau_minus)\n",
    "\n",
    "    return dw\n",
    "\n",
    "\n",
    "import doctest\n",
    "\n",
    "doctest.testmod(verbose=True)\n",
    "\n",
    "# Plotting\n",
    "delta_t = np.linspace(-100, 100, 1000)\n",
    "dw = stdp_window(delta_t)\n",
    "\n",
    "plotting.plot_stdp_window(delta_t, dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Implementing STDP in a LIF Neuron\n",
    "\n",
    "To simulate STDP efficiently, we use an **online** implementation with **synaptic traces**. Instead of iterating through all pairs of spikes, we maintain traces that decay over time:\n",
    "\n",
    "- **Pre-synaptic trace ($x_i$)**: Increases when a pre-synaptic spike arrives, decays with $\\tau_+$.\n",
    "- **Post-synaptic trace ($y$)**: Increases when the post-synaptic neuron fires, decays with $\\tau_-$.\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "1. **On Pre-synaptic spike** (at synapse $i$):\n",
    "\n",
    "   - Update trace: $x_i \\leftarrow x_i + 1$\n",
    "   - **LTD**: $w_i \\leftarrow w_i - A_- \\cdot y$ (Depress weight based on recent post-synaptic activity)\n",
    "\n",
    "2. **On Post-synaptic spike**:\n",
    "   - Update trace: $y \\leftarrow y + 1$\n",
    "   - **LTP**: For all synapses $i$, $w_i \\leftarrow w_i + A_+ \\cdot x_i$ (Potentiate weights based on recent pre-synaptic activity)\n",
    "\n",
    "Let's implement a simple LIF neuron with these plastic synapses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuronSTDP:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_inputs: int,\n",
    "        tau_m: float = 20.0,\n",
    "        v_rest: float = -65.0,\n",
    "        v_reset: float = -65.0,\n",
    "        v_thresh: float = -50.0,\n",
    "        tau_plus: float = 20.0,\n",
    "        tau_minus: float = 20.0,\n",
    "        A_plus: float = 0.01,\n",
    "        A_minus: float = 0.012,\n",
    "        w_max: float = 1.0,\n",
    "    ):\n",
    "        # Neuron parameters\n",
    "        self.tau_m = tau_m  # Membrane time constant (ms)\n",
    "        self.v_rest = v_rest  # Resting potential (mV)\n",
    "        self.v_reset = v_reset  # Reset potential (mV)\n",
    "        self.v_thresh = v_thresh  # Threshold (mV)\n",
    "\n",
    "        # STDP parameters\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "        self.A_plus = A_plus\n",
    "        self.A_minus = A_minus\n",
    "        self.w_max = w_max\n",
    "\n",
    "        # State variables\n",
    "        self.v = self.v_rest\n",
    "        self.n_inputs = n_inputs\n",
    "        self.weights = np.random.uniform(0.1, 0.5, n_inputs)  # Initial random weights\n",
    "\n",
    "        # Traces\n",
    "        self.x_trace = np.zeros(n_inputs)  # Pre-synaptic traces\n",
    "        self.y_trace = 0.0  # Post-synaptic trace\n",
    "\n",
    "    def step(self, dt, input_spikes):\n",
    "        \"\"\"\n",
    "        Simulate one time step.\n",
    "\n",
    "        Args:\n",
    "            dt (float): Time step (ms)\n",
    "            input_spikes (np.array): Boolean array of size n_inputs, True if spike occurred\n",
    "        \"\"\"\n",
    "        # 1. Decay traces\n",
    "        self.x_trace *= np.exp(-dt / self.tau_plus)\n",
    "        self.y_trace *= np.exp(-dt / self.tau_minus)\n",
    "\n",
    "        # 2. Handle Pre-synaptic spikes (LTD and Trace update)\n",
    "        # Indices of neurons that spiked\n",
    "        pre_spike_indices = np.where(input_spikes)[0]\n",
    "\n",
    "        if len(pre_spike_indices) > 0:\n",
    "            # Update pre-synaptic traces\n",
    "            self.x_trace[pre_spike_indices] += 1.0\n",
    "\n",
    "            # LTD: Weaken weights if post-synaptic neuron fired recently (high y_trace)\n",
    "            # w = w - A_minus * y_trace\n",
    "            self.weights[pre_spike_indices] -= self.A_minus * self.y_trace\n",
    "            self.weights = np.clip(self.weights, 0, self.w_max)\n",
    "\n",
    "        # 3. Update Membrane Potential (LIF dynamics)\n",
    "        # I_syn = sum(w_i * spike_i) (simplified current injection)\n",
    "        # In a real simulation, we might use conductance-based or current-based synapses with time constants.\n",
    "        # Here we assume instantaneous current injection for simplicity.\n",
    "        i_syn = np.sum(self.weights[input_spikes]) * 10.0  # Scaling factor for current\n",
    "\n",
    "        # dV = (-(V - V_rest) + R*I) / tau_m * dt\n",
    "        # Assuming R=1 for simplicity\n",
    "        dv = (-(self.v - self.v_rest) + i_syn) / self.tau_m * dt\n",
    "        self.v += dv\n",
    "\n",
    "        # 4. Check for Post-synaptic spike\n",
    "        post_spike = False\n",
    "        if self.v >= self.v_thresh:\n",
    "            post_spike = True\n",
    "            self.v = self.v_reset\n",
    "\n",
    "            # Update post-synaptic trace\n",
    "            self.y_trace += 1.0\n",
    "\n",
    "            # LTP: Strengthen weights if pre-synaptic neurons fired recently (high x_trace)\n",
    "            # w = w + A_plus * x_trace\n",
    "            self.weights += self.A_plus * self.x_trace\n",
    "            self.weights = np.clip(self.weights, 0, self.w_max)\n",
    "\n",
    "        return post_spike, self.v, self.weights.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Experiment: Pattern Learning\n",
    "\n",
    "We will now demonstrate how STDP allows a neuron to learn a repeating pattern hidden in noise.\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "- **100 Input Neurons** connected to 1 Output Neuron.\n",
    "- **Input:** Most of the time, inputs are random Poisson noise.\n",
    "- **Pattern:** Every 200 ms, a specific \"frozen\" pattern of spikes (lasting 50 ms) is presented to the inputs.\n",
    "- **Goal:** The neuron should learn to recognize this pattern by strengthening the weights of synapses that participate in the pattern and weakening others.\n",
    "\n",
    "We expect the neuron to eventually fire reliably when the pattern is presented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation Parameters\n",
    "dt = 1.0  # Time step (ms)\n",
    "T = 10000  # Total duration (ms)\n",
    "n_steps = int(T / dt)\n",
    "n_inputs = 100\n",
    "input_rate = 0.01  # Probability of spike per ms (10 Hz)\n",
    "pattern_rate = 0.08  # Probability of spike in pattern (80 Hz)\n",
    "\n",
    "# Generate Input Spikes\n",
    "# 1. Background noise\n",
    "input_spikes = np.random.rand(n_steps, n_inputs) < (input_rate * dt)\n",
    "\n",
    "# 2. Create a frozen pattern (50 ms duration)\n",
    "pattern_duration = 50\n",
    "pattern_template = np.random.rand(pattern_duration, n_inputs) < (pattern_rate * dt)\n",
    "\n",
    "# 3. Embed pattern at random intervals\n",
    "pattern_times = []\n",
    "current_time = 200\n",
    "while current_time < n_steps - pattern_duration:\n",
    "    input_spikes[current_time : current_time + pattern_duration] = pattern_template\n",
    "    pattern_times.append(current_time)\n",
    "    current_time += 200 + np.random.randint(0, 100)  # Repeat every ~250ms\n",
    "\n",
    "print(f\"Pattern embedded {len(pattern_times)} times.\")\n",
    "\n",
    "plotting.plot_input_spike_train(input_spikes, pattern_times, pattern_duration, n_steps, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neuron\n",
    "neuron = LIFNeuronSTDP(n_inputs, A_plus=0.015, A_minus=0.01, w_max=0.5, v_thresh=-50)\n",
    "\n",
    "# Recording arrays\n",
    "v_rec = np.zeros(n_steps)\n",
    "weights_rec = np.zeros((n_steps // 100, n_inputs))  # Record weights every 100 steps\n",
    "output_spikes = []\n",
    "\n",
    "# Run Simulation\n",
    "print(\"Running simulation...\")\n",
    "for t in range(n_steps):\n",
    "    spike, v, w = neuron.step(dt, input_spikes[t])\n",
    "\n",
    "    v_rec[t] = v\n",
    "    if spike:\n",
    "        output_spikes.append(t)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        weights_rec[t // 100] = w\n",
    "\n",
    "print(f\"Simulation complete. Total output spikes: {len(output_spikes)}\")\n",
    "\n",
    "# Identify neurons that are active in the pattern\n",
    "pattern_activity = np.sum(pattern_template, axis=0)\n",
    "pattern_indices = np.where(pattern_activity > 0)[0]\n",
    "non_pattern_indices = np.where(pattern_activity == 0)[0]\n",
    "\n",
    "plotting.plot_stdp_simulation_results(\n",
    "    v_rec,\n",
    "    output_spikes,\n",
    "    input_spikes,\n",
    "    weights_rec,\n",
    "    pattern_times,\n",
    "    pattern_duration,\n",
    "    pattern_indices,\n",
    "    non_pattern_indices,\n",
    "    dt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. ðŸŽ“ Exercises\n",
    "\n",
    "Now it's your turn to explore the properties of STDP!\n",
    "\n",
    "### Exercise 1: The Balance of Power\n",
    "\n",
    "In the simulation above, we set $A_- > A_+$ (slightly). This bias towards depression is often important for stability, ensuring weights don't explode.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Modify the `LIFNeuronSTDP` initialization to set `A_plus` significantly larger than `A_minus` (e.g., `A_plus=0.015`, `A_minus=0.01`).\n",
    "2.  Run the simulation again.\n",
    "3.  Plot the weight evolution.\n",
    "\n",
    "**Question:** What happens to the weights of the \"noise\" synapses (those not involved in the pattern)? Do they stay low?\n",
    "\n",
    "### Exercise 2: Learning Speed\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Increase both `A_plus` and `A_minus` by a factor of 5.\n",
    "2.  Run the simulation.\n",
    "\n",
    "**Question:** Does the neuron learn the pattern faster? Is the final weight state stable, or does it fluctuate wildly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Modify A_plus > A_minus\n",
    "\n",
    "# TODO: Initialize neuron with new parameters (A_plus=0.015, A_minus=0.01)\n",
    "\n",
    "# Record weights every 100 steps\n",
    "weights_rec_ex1 = np.zeros((n_steps // 100, n_inputs))\n",
    "# TODO: Run Simulation\n",
    "\n",
    "# TODO: Plot the weight evolution\n",
    "\n",
    "# Plot the weight evolution\n",
    "plotting.plot_weight_evolution(\n",
    "    weights_rec_ex1,\n",
    "    pattern_indices,\n",
    "    non_pattern_indices,\n",
    "    n_steps,\n",
    "    dt,\n",
    "    title=\"Exercise 1: Weight Evolution (A+ > A-)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: High Learning Rate\n",
    "\n",
    "# TODO: Initialize neuron with 5x learning rates\n",
    "\n",
    "weights_rec_ex2 = np.zeros((n_steps // 100, n_inputs))\n",
    "# TODO: Run simulation\n",
    "\n",
    "# TODO: Plot the weight evolution\n",
    "\n",
    "# Plot results\n",
    "plotting.plot_weight_evolution(\n",
    "    weights_rec_ex2,\n",
    "    pattern_indices,\n",
    "    non_pattern_indices,\n",
    "    n_steps,\n",
    "    dt,\n",
    "    title=\"Exercise 2: Weight Evolution (High Learning Rate)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Exercise 3: Homeostasis via Synaptic Scaling\n",
    "\n",
    "Hebbian learning can sometimes lead to \"runaway\" dynamics where weights either explode to the maximum or vanish to zero. To prevent this, the brain uses **homeostatic mechanisms** to keep neuronal activity stable.\n",
    "\n",
    "One such mechanism is **Synaptic Scaling**, where the total synaptic weight received by a neuron is kept constant. This introduces competition: if one synapse gets stronger, others must get weaker.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Modify the simulation loop to include a normalization step.\n",
    "2.  After every weight update (or every $N$ steps), scale the weights so that their sum equals a target value $W_{target}$.\n",
    "    $$ \\mathbf{w} \\leftarrow \\mathbf{w} \\frac{W\\_{target}}{\\sum \\mathbf{w}} $$\n",
    "3.  Run the simulation with `A_plus` > `A_minus` (which caused instability in Exercise 1).\n",
    "4.  Does synaptic scaling stabilize the learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Synaptic Scaling\n",
    "\n",
    "# TODO: Initialize neuron with unstable parameters (A_plus > A_minus) and a target total weight\n",
    "\n",
    "weights_rec_ex3 = np.zeros((n_steps // 100, n_inputs))\n",
    "# TODO: Run Simulation with synaptic scaling step\n",
    "\n",
    "# TODO: Plot results\n",
    "\n",
    "# Plot results\n",
    "plotting.plot_weight_evolution(\n",
    "    weights_rec_ex3,\n",
    "    pattern_indices,\n",
    "    non_pattern_indices,\n",
    "    n_steps,\n",
    "    dt,\n",
    "    title=\"Exercise 3: Weight Evolution (Synaptic Scaling)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Exercise 4: Anti-Hebbian Learning\n",
    "\n",
    "In some neural circuits (e.g., the cerebellum-like structures), the learning rule is reversed:\n",
    "\n",
    "- **Pre-before-Post** leads to **LTD**.\n",
    "- **Post-before-Pre** leads to **LTP**.\n",
    "\n",
    "This is known as **Anti-Hebbian learning**. It is often used for predictive cancellation of expected sensory inputs.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Modify the `stdp_window` function (or create a new one) to implement Anti-Hebbian learning.\n",
    "2.  Plot the STDP window to verify the shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Anti-Hebbian STDP Window\n",
    "\n",
    "# TODO: Implement anti-Hebbian window function\n",
    "def anti_hebbian_window(delta_t, A_plus=0.01, A_minus=0.01, tau_plus=20, tau_minus=20): ...\n",
    "\n",
    "\n",
    "# TODO: Plot the window\n",
    "\n",
    "# Plotting\n",
    "delta_t = np.linspace(-100, 100, 1000)\n",
    "dw_anti = anti_hebbian_window(delta_t)\n",
    "\n",
    "# Note: We keep this plot here as it's specific to the exercise\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=delta_t,\n",
    "        y=dw_anti,\n",
    "        mode=\"lines\",\n",
    "        name=\"Anti-Hebbian Window\",\n",
    "        line=dict(color=\"purple\", width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "fig.update_layout(title=\"Anti-Hebbian Learning Window\", xaxis_title=\"Î”t (ms)\", yaxis_title=\"Î”w\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Exercise 5: Neuromodulated STDP (Global Signals)\n",
    "\n",
    "In the brain, learning is often gated by global neuromodulatory signals (like dopamine, acetylcholine, or norepinephrine) that signal reward, novelty, or surprise. This is often modeled as a **three-factor learning rule**:\n",
    "\n",
    "$$ \\Delta w \\propto M \\cdot \\text{STDP}(\\Delta t) $$\n",
    "\n",
    "Where $M$ is a global neuromodulatory signal. If $M=0$, no learning occurs. If $M>0$, learning is enabled or amplified.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Think about how you would modify the `LIFNeuronSTDP` class to include a global `reward` signal.\n",
    "2.  Implement a simple simulation where learning is only enabled during specific \"rewarded\" windows (e.g., when the pattern is present).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5: Neuromodulated STDP\n",
    "\n",
    "# TODO: Initialize neuron\n",
    "\n",
    "# TODO: Define reward signal\n",
    "\n",
    "weights_rec_gated = np.zeros((n_steps // 100, n_inputs))\n",
    "# TODO: Run Simulation with gated learning\n",
    "\n",
    "# TODO: Plot results\n",
    "\n",
    "# Plot results\n",
    "plotting.plot_weight_evolution(\n",
    "    weights_rec_gated,\n",
    "    pattern_indices,\n",
    "    non_pattern_indices,\n",
    "    n_steps,\n",
    "    dt,\n",
    "    title=\"Exercise 5: Gated STDP (Learning only during pattern)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we implemented the STDP learning rule and applied it to a Leaky Integrate-and-Fire neuron. We observed that:\n",
    "\n",
    "1.  **STDP modifies synaptic weights** based on the precise timing of pre- and post-synaptic spikes.\n",
    "2.  **Pattern Learning**: The neuron learned to detect a repeating spatio-temporal pattern hidden in noise. Synapses corresponding to the pattern were strengthened (LTP), while others were weakened (LTD) or remained low.\n",
    "3.  **Selectivity**: After learning, the neuron fires reliably when the pattern is presented, acting as a pattern detector.\n",
    "\n",
    "This mechanism is thought to be a fundamental way the brain learns to recognize features and sequences in sensory input.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
