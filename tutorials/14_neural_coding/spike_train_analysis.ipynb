{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Data Analysis: Spike Trains and Population Coding\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- Generate synthetic spike trains using Poisson processes.\n",
    "- Visualize neural activity using raster plots.\n",
    "- Calculate basic statistics like firing rates and Inter-Spike Intervals (ISIs).\n",
    "- Compute variability measures such as the Coefficient of Variation (CV) and Fano Factor.\n",
    "- Apply Information Theory concepts like Entropy and Mutual Information to neural data.\n",
    "- Implement a population decoder to classify stimuli based on neural activity.\n",
    "- Use standard Machine Learning libraries (`scikit-learn`) for neural decoding.\n",
    "- Understand the difference between single-neuron and population coding.\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- Basic Python programming (lists, loops, functions).\n",
    "- Familiarity with `numpy` arrays.\n",
    "- Basic understanding of probability (distributions, means).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Neurons communicate using discrete electrical pulses called **action potentials** or **spikes**. A sequence of spikes over time is called a **spike train**. Because the exact timing of spikes can be irregular, we often treat them as stochastic processes.\n",
    "\n",
    "In this tutorial, we will explore how to analyze these spike trains. We will start by generating synthetic data, then move on to visualizing it, calculating statistics, and finally, using the activity of a population of neurons to \"decode\" what stimulus the brain is seeing.\n",
    "\n",
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from neuroai import plotting\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spike_train(rate: float, duration: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates spike times for a Poisson process.\n",
    "\n",
    "    Args:\n",
    "        rate (float): Firing rate in Hz.\n",
    "        duration (float): Duration in seconds.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of spike times.\n",
    "    \"\"\"\n",
    "    if rate <= 0:\n",
    "        return np.array([])\n",
    "\n",
    "    # Generate ISIs from an exponential distribution\n",
    "    # We generate a buffer of spikes to ensure we cover the full duration\n",
    "    expected_spikes = int(rate * duration * 2.0) + 10\n",
    "    isis = np.random.exponential(1.0 / rate, expected_spikes)\n",
    "    spike_times = np.cumsum(isis)\n",
    "\n",
    "    # Keep only spikes within the duration\n",
    "    return spike_times[spike_times < duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_neurons = 20\n",
    "n_trials = 50\n",
    "duration = 1.0  # seconds\n",
    "\n",
    "# Define firing rates for two stimuli\n",
    "# We make the difference between stimuli subtle to highlight the benefit of population coding.\n",
    "# Stimulus A: Neurons 0-9 have slightly higher rates\n",
    "# Stimulus B: Neurons 10-19 have slightly higher rates\n",
    "base_rate = 15.0  # Hz\n",
    "delta_rate = 5.0  # Hz difference\n",
    "\n",
    "rates_A = np.ones(n_neurons) * base_rate\n",
    "rates_A[:10] += delta_rate  # Neurons 0-9 fire at 20Hz\n",
    "\n",
    "rates_B = np.ones(n_neurons) * base_rate\n",
    "rates_B[10:] += delta_rate  # Neurons 10-19 fire at 20Hz\n",
    "\n",
    "# Generate data structure: data[stimulus][trial][neuron_idx] = spike_times_array\n",
    "data = {\"A\": [], \"B\": []}\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    # Stimulus A trials\n",
    "    trial_spikes_A = []\n",
    "    for n in range(n_neurons):\n",
    "        trial_spikes_A.append(generate_spike_train(rates_A[n], duration))\n",
    "    data[\"A\"].append(trial_spikes_A)\n",
    "\n",
    "    # Stimulus B trials\n",
    "    trial_spikes_B = []\n",
    "    for n in range(n_neurons):\n",
    "        trial_spikes_B.append(generate_spike_train(rates_B[n], duration))\n",
    "    data[\"B\"].append(trial_spikes_B)\n",
    "\n",
    "print(f\"Generated data for {n_neurons} neurons, {n_trials} trials per stimulus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Visualizing Raster Plots\n",
    "\n",
    "Before analysing, it's always good to look at the data. A raster plot shows spike times for each neuron in a single trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one trial from Stimulus A\n",
    "plotting.plot_raster_from_list(data[\"A\"][0], \"Raster Plot - Stimulus A (Trial 0)\")\n",
    "\n",
    "# Plot one trial from Stimulus B\n",
    "plotting.plot_raster_from_list(data[\"B\"][0], \"Raster Plot - Stimulus B (Trial 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### ðŸ§  Let's think about it!\n",
    "\n",
    "Look at the raster plots above.\n",
    "\n",
    "1. Can you see a difference between Stimulus A and Stimulus B just by eye?\n",
    "2. Are there any neurons that seem to fire for both stimuli?\n",
    "3. How variable is the firing? Do the spikes look perfectly regular (like a clock) or random?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Calculating Firing Rates\n",
    "\n",
    "The firing rate is the number of spikes per unit time (usually spikes/second or Hz).\n",
    "\n",
    "### ðŸŽ“ Exercise 1: Calculate Firing Rate\n",
    "\n",
    "**Task:**\n",
    "Write a function to calculate the average firing rate of a neuron across a trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_firing_rate(spike_times: np.ndarray, duration: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates firing rate in Hz.\n",
    "\n",
    "    Args:\n",
    "        spike_times (np.array): Array of spike times in seconds.\n",
    "        duration (float): Duration of the recording in seconds.\n",
    "\n",
    "    Returns:\n",
    "        float: Firing rate in Hz.\n",
    "\n",
    "    Examples:\n",
    "        >>> spikes = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        >>> calculate_firing_rate(spikes, 1.0)\n",
    "        5.0\n",
    "        >>> calculate_firing_rate(spikes, 0.5)\n",
    "        10.0\n",
    "        >>> calculate_firing_rate(np.array([]), 1.0)\n",
    "        0.0\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Firing rate = number of spikes / duration\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "doctest.run_docstring_examples(calculate_firing_rate, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### âœ… Solution - Exercise 1\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution</summary>\n",
    "\n",
    "```python\n",
    "def calculate_firing_rate(spike_times: np.ndarray, duration: float) -> float:\n",
    "    num_spikes = len(spike_times)\n",
    "    return num_spikes / duration\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Inter-Spike Intervals (ISIs)\n",
    "\n",
    "The Inter-Spike Interval (ISI) is the time difference between consecutive spikes. The distribution of ISIs can tell us about the regularity of firing.\n",
    "\n",
    "### ðŸŽ“ Exercise 2: Calculate ISIs\n",
    "\n",
    "**Task:**\n",
    "Calculate the ISIs for a specific neuron across all trials of Stimulus A and plot the histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isis(spike_times: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates inter-spike intervals (time between consecutive spikes).\n",
    "\n",
    "    Args:\n",
    "        spike_times (np.array): Sorted spike times.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of ISIs.\n",
    "\n",
    "    Examples:\n",
    "        >>> spikes = np.array([0.1, 0.2, 0.35, 0.6])\n",
    "        >>> get_isis(spikes)\n",
    "        array([0.1 , 0.15, 0.25])\n",
    "        >>> get_isis(np.array([0.5]))  # Single spike has no ISIs\n",
    "        array([], dtype=float64)\n",
    "        >>> get_isis(np.array([]))  # No spikes\n",
    "        array([], dtype=float64)\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Use np.diff() to compute differences between consecutive elements\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "doctest.run_docstring_examples(get_isis, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_isis = []\n",
    "neuron_idx = 0\n",
    "for trial in data[\"A\"]:\n",
    "    spikes = trial[neuron_idx]\n",
    "    isis = get_isis(spikes)\n",
    "    all_isis.extend(isis)\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=all_isis, nbins=50, labels={\"x\": \"ISI (s)\", \"y\": \"Count\"}, title=\"ISI Distribution\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### ðŸ§  Let's think about it!\n",
    "\n",
    "1. What is the shape of the ISI distribution? Does it look like an exponential distribution?\n",
    "2. What does an exponential ISI distribution imply about the underlying spike generation process? (Hint: Think about Poisson processes).\n",
    "3. If the neuron was firing perfectly regularly (like a metronome), what would the ISI histogram look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. Variability Statistics\n",
    "\n",
    "Neurons in the brain are often noisy. Two common measures to quantify this variability are the **Coefficient of Variation (CV)** of the ISIs and the **Fano Factor** of the spike counts.\n",
    "\n",
    "### Coefficient of Variation (CV)\n",
    "\n",
    "The CV measures the irregularity of spike timing. It is defined as the ratio of the standard deviation of the ISIs to the mean ISI:\n",
    "\n",
    "$$ CV = \\frac{\\sigma*{ISI}}{\\mu*{ISI}} $$\n",
    "\n",
    "- **CV â‰ˆ 0**: Extremely regular firing (like a clock).\n",
    "- **CV â‰ˆ 1**: Poisson process (random spikes).\n",
    "- **CV > 1**: Bursty firing.\n",
    "\n",
    "**Read more:** [Coefficient of variation (Wikipedia)](https://en.wikipedia.org/wiki/Coefficient_of_variation)\n",
    "\n",
    "### Fano Factor\n",
    "\n",
    "The Fano Factor measures the variability of the spike count over repeated trials. It is defined as the variance of the spike count divided by the mean spike count:\n",
    "\n",
    "$$ F = \\frac{\\sigma^2*{counts}}{\\mu*{counts}} $$\n",
    "\n",
    "- **F â‰ˆ 1**: Poisson process.\n",
    "- **F < 1**: Regular firing.\n",
    "- **F > 1**: Highly variable (bursty or doubly stochastic).\n",
    "\n",
    "**Read more:** [Fano factor (Wikipedia)](https://en.wikipedia.org/wiki/Fano_factor)\n",
    "\n",
    "### ðŸŽ“ Exercise 3: Calculate CV and Fano Factor\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Write a function to calculate the CV of a spike train.\n",
    "2.  Write a function to calculate the Fano Factor for a neuron across multiple trials.\n",
    "3.  Calculate these values for our generated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cv(isis: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Coefficient of Variation (CV) of ISIs.\n",
    "\n",
    "    CV = std(ISIs) / mean(ISIs)\n",
    "    - CV â‰ˆ 0: Regular firing\n",
    "    - CV â‰ˆ 1: Poisson process\n",
    "    - CV > 1: Bursty firing\n",
    "\n",
    "    Args:\n",
    "        isis (np.array): Array of Inter-Spike Intervals.\n",
    "\n",
    "    Returns:\n",
    "        float: CV value.\n",
    "\n",
    "    Examples:\n",
    "        >>> isis = np.array([0.05, 0.05, 0.05, 0.05])  # Regular firing\n",
    "        >>> calculate_cv(isis)\n",
    "        np.float64(0.0)\n",
    "        >>> isis = np.array([0.1, 0.2, 0.3, 0.4])  # Variable ISIs\n",
    "        >>> round(calculate_cv(isis), 4)\n",
    "        np.float64(0.4472)\n",
    "        >>> calculate_cv(np.array([0.1]))  # Single ISI\n",
    "        np.float64(0.0)\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: CV = standard deviation / mean\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_fano_factor(spike_counts: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Fano Factor of spike counts across trials.\n",
    "\n",
    "    Fano Factor = variance(counts) / mean(counts)\n",
    "    - F â‰ˆ 1: Poisson process\n",
    "    - F < 1: Regular firing\n",
    "    - F > 1: Bursty firing\n",
    "\n",
    "    Args:\n",
    "        spike_counts (np.array): Array of spike counts across trials.\n",
    "\n",
    "    Returns:\n",
    "        float: Fano Factor.\n",
    "\n",
    "    Examples:\n",
    "        >>> counts = np.array([10, 10, 10, 10])  # No variability\n",
    "        >>> calculate_fano_factor(counts)\n",
    "        np.float64(0.0)\n",
    "        >>> counts = np.array([8, 10, 12, 10])  # Some variability\n",
    "        >>> calculate_fano_factor(counts)\n",
    "        np.float64(0.2)\n",
    "        >>> counts = np.array([5, 15, 5, 15])  # High variability\n",
    "        >>> calculate_fano_factor(counts)\n",
    "        np.float64(2.5)\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Fano Factor = variance / mean\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementations\n",
    "doctest.run_docstring_examples(calculate_cv, globals())\n",
    "doctest.run_docstring_examples(calculate_fano_factor, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 6. Information Theory\n",
    "\n",
    "Information theory provides a powerful framework for quantifying how much information a neuron carries about a stimulus.\n",
    "\n",
    "### Entropy\n",
    "\n",
    "Entropy ($H$) measures the uncertainty or \"surprise\" of a random variable. For a discrete random variable $X$ with probability mass function $P(x)$:\n",
    "\n",
    "<!-- prettier-ignore -->\n",
    "$$ H(X) = - \\sum_{x} P(x) \\log_2 P(x) $$\n",
    "\n",
    "### Mutual Information\n",
    "\n",
    "Mutual Information ($MI$) measures how much knowing one variable reduces uncertainty about another. In our case, we want to know how much information the **spike count** ($R$) carries about the **stimulus** ($S$).\n",
    "\n",
    "$$ MI(S; R) = H(R) - H(R|S) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $H(R)$ is the total entropy of the response (spike counts).\n",
    "- $H(R|S)$ is the conditional entropy of the response given the stimulus (noise entropy).\n",
    "\n",
    "### ðŸŽ“ Exercise 4: Calculate Mutual Information\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Discretize the spike counts for a single neuron (e.g., Neuron 0).\n",
    "2.  Calculate the probability distribution of spike counts $P(r)$.\n",
    "3.  Calculate the conditional probability distributions $P(r|s)$ for each stimulus.\n",
    "4.  Compute the Mutual Information.\n",
    "\n",
    "_Hint: You can use `np.histogram` to estimate probabilities._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(prob_dist: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates Shannon entropy in bits.\n",
    "\n",
    "    Entropy measures the uncertainty or \"surprise\" of a probability distribution.\n",
    "    H(X) = -sum(P(x) * log2(P(x)))\n",
    "\n",
    "    Args:\n",
    "        prob_dist (np.array): Probability distribution (must sum to 1).\n",
    "\n",
    "    Returns:\n",
    "        float: Entropy in bits.\n",
    "\n",
    "    Examples:\n",
    "        >>> p = np.array([0.5, 0.5])  # Maximum entropy for 2 outcomes\n",
    "        >>> calculate_entropy(p)\n",
    "        1.0\n",
    "        >>> p = np.array([1.0, 0.0])  # No uncertainty (zero entropy)\n",
    "        >>> calculate_entropy(p)\n",
    "        0.0\n",
    "        >>> p = np.array([0.25, 0.25, 0.25, 0.25])  # Uniform over 4\n",
    "        >>> calculate_entropy(p)\n",
    "        2.0\n",
    "        >>> p = np.array([0.9, 0.1])  # Low entropy (high certainty)\n",
    "        >>> round(calculate_entropy(p), 4)\n",
    "        0.469\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Filter out zero probabilities to avoid log(0)\n",
    "    # Use np.log2 for bits\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_mutual_information(counts_A: np.ndarray, counts_B: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates Mutual Information between Stimulus (A/B) and Response (spike counts).\n",
    "\n",
    "    MI(S; R) = H(R) - H(R|S)\n",
    "\n",
    "    Assumes stimuli are presented with equal probability P(A)=P(B)=0.5.\n",
    "\n",
    "    Args:\n",
    "        counts_A (np.array): Spike counts for stimulus A trials.\n",
    "        counts_B (np.array): Spike counts for stimulus B trials.\n",
    "\n",
    "    Returns:\n",
    "        float: Mutual Information in bits.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Non-overlapping responses = maximum information (1 bit)\n",
    "        >>> counts_A = np.array([10, 10, 10, 10])\n",
    "        >>> counts_B = np.array([20, 20, 20, 20])\n",
    "        >>> calculate_mutual_information(counts_A, counts_B)\n",
    "        1.0\n",
    "        >>> # Identical responses = zero information\n",
    "        >>> counts_A = np.array([15, 15, 15, 15])\n",
    "        >>> counts_B = np.array([15, 15, 15, 15])\n",
    "        >>> calculate_mutual_information(counts_A, counts_B)\n",
    "        0.0\n",
    "        >>> # Partial overlap = partial information (0.5 bits)\n",
    "        >>> counts_A = np.array([10, 10, 11, 11])  # Half at 10, half at 11\n",
    "        >>> counts_B = np.array([11, 11, 12, 12])  # Half at 11, half at 12\n",
    "        >>> calculate_mutual_information(counts_A, counts_B)\n",
    "        0.5\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Steps:\n",
    "    # 1. Define bins for response histogram\n",
    "    # 2. Calculate P(r|A) and P(r|B) using np.histogram with density=True\n",
    "    # 3. Calculate P(r) = 0.5 * P(r|A) + 0.5 * P(r|B)\n",
    "    # 4. Compute H(R), H(R|A), H(R|B)\n",
    "    # 5. MI = H(R) - 0.5*H(R|A) - 0.5*H(R|B)\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementations\n",
    "doctest.run_docstring_examples(calculate_entropy, globals())\n",
    "doctest.run_docstring_examples(calculate_mutual_information, globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 7. Population Decoding\n",
    "\n",
    "Can we determine which stimulus was presented just by looking at the population activity?\n",
    "\n",
    "We will implement a simple **template matching** decoder.\n",
    "\n",
    "1. Calculate the \"template\" population vector for each stimulus (average firing rates across training trials).\n",
    "2. For a new \"test\" trial, calculate its population vector.\n",
    "3. Compare the test vector to the templates (e.g., using Euclidean distance or correlation).\n",
    "4. Assign the stimulus label of the closest template.\n",
    "\n",
    "### ðŸŽ“ Exercise 5: Population Decoder\n",
    "\n",
    "**Task:**\n",
    "Split the data into training (first 40 trials) and testing (last 10 trials). Train the decoder and evaluate its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_population_vector(trial_data: list[np.ndarray], duration: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an array of firing rates for all neurons in a trial.\n",
    "\n",
    "    This creates a \"population vector\" - a snapshot of activity across all neurons\n",
    "    that can be used for decoding which stimulus was presented.\n",
    "\n",
    "    Args:\n",
    "        trial_data (list): List of spike time arrays for each neuron.\n",
    "        duration (float): Duration of the trial in seconds.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of firing rates (one per neuron).\n",
    "\n",
    "    Examples:\n",
    "        >>> trial = [np.array([0.1, 0.2]), np.array([0.5])]\n",
    "        >>> get_population_vector(trial, 1.0)\n",
    "        array([2., 1.])\n",
    "        >>> trial = [np.array([0.1, 0.2, 0.3]), np.array([]), np.array([0.5, 0.6])]\n",
    "        >>> get_population_vector(trial, 1.0)\n",
    "        array([3., 0., 2.])\n",
    "        >>> trial = [np.array([0.1, 0.2])]\n",
    "        >>> get_population_vector(trial, 0.5)  # Shorter duration = higher rate\n",
    "        array([4.])\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: Loop through each neuron's spike times and calculate firing rate\n",
    "    # Or use a list comprehension with len(spikes) / duration\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "doctest.run_docstring_examples(get_population_vector, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "n_train = 40\n",
    "train_A = data[\"A\"][:n_train]\n",
    "test_A = data[\"A\"][n_train:]\n",
    "train_B = data[\"B\"][:n_train]\n",
    "test_B = data[\"B\"][n_train:]\n",
    "\n",
    "\n",
    "# 1. Calculate Templates (Mean population vectors)\n",
    "mean_vector_A = np.mean([get_population_vector(trial, duration) for trial in train_A], axis=0)\n",
    "mean_vector_B = np.mean([get_population_vector(trial, duration) for trial in train_B], axis=0)\n",
    "\n",
    "\n",
    "# 2. Decode Test Trials\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Test with Stimulus A trials\n",
    "for trial in test_A:\n",
    "    vec = get_population_vector(trial, duration)\n",
    "    dist_A = np.linalg.norm(vec - mean_vector_A)\n",
    "    dist_B = np.linalg.norm(vec - mean_vector_B)\n",
    "    if dist_A < dist_B:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Test with Stimulus B trials\n",
    "for trial in test_B:\n",
    "    vec = get_population_vector(trial, duration)\n",
    "    dist_A = np.linalg.norm(vec - mean_vector_A)\n",
    "    dist_B = np.linalg.norm(vec - mean_vector_B)\n",
    "    if dist_B < dist_A:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Decoding Accuracy: {correct / total * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Now that you have a working decoder, try exploring the following:\n",
    "\n",
    "1. **Vary the number of neurons**: How does decoding accuracy change if you only use 5 neurons? Or 1 neuron?\n",
    "2. **Vary the noise**: In the data generation function, try making the firing rates of the two stimuli more similar (e.g., 20Hz vs 22Hz). How does this affect accuracy?\n",
    "3. **Time window**: What if you only use the first 100ms of the trial? Does accuracy drop?\n",
    "4. **Different Decoder**: Try using a correlation-based metric instead of Euclidean distance. Does it work better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 8. Decoding with Machine Learning\n",
    "\n",
    "While our manual template matcher works, modern neuroscience relies heavily on standard Machine Learning libraries. Let's use `scikit-learn` to train a classifier.\n",
    "\n",
    "We will use **Logistic Regression**, a linear classifier that learns a weight for each neuron.\n",
    "\n",
    "### ðŸŽ“ Exercise 6: Scikit-Learn Decoder\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Prepare the data matrix `X` (n_samples, n_features) and labels `y` (n_samples).\n",
    "    - `X`: Each row is a trial, each column is a neuron's firing rate.\n",
    "    - `y`: 0 for Stimulus A, 1 for Stimulus B.\n",
    "2.  Split into training and testing sets.\n",
    "3.  Train a `LogisticRegression` model.\n",
    "4.  Calculate accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_sklearn(data: dict, duration: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepares spike train data for scikit-learn classifiers.\n",
    "\n",
    "    Converts spike train data into feature matrix X and label vector y.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Dictionary with keys 'A' and 'B', each containing trial data.\n",
    "        duration (float): Duration of each trial in seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y) where X is (n_samples, n_features) and y is (n_samples,)\n",
    "            - X: Each row is a trial's population vector (firing rates)\n",
    "            - y: Labels (0 for stimulus A, 1 for stimulus B)\n",
    "\n",
    "    Examples:\n",
    "        >>> # Create minimal test data\n",
    "        >>> test_data = {\n",
    "        ...     'A': [[np.array([0.1, 0.2]), np.array([0.5])]],  # 1 trial, 2 neurons\n",
    "        ...     'B': [[np.array([0.3]), np.array([0.4, 0.5, 0.6])]]  # 1 trial, 2 neurons\n",
    "        ... }\n",
    "        >>> X, y = prepare_data_for_sklearn(test_data, 1.0)\n",
    "        >>> X.shape\n",
    "        (2, 2)\n",
    "        >>> list(y)\n",
    "        [0, 1]\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Create empty lists for X and y\n",
    "    # 2. Loop through data[\"A\"] trials, append population vectors to X, append 0 to y\n",
    "    # 3. Loop through data[\"B\"] trials, append population vectors to X, append 1 to y\n",
    "    # 4. Convert to numpy arrays\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def train_and_evaluate_decoder(\n",
    "    X: np.ndarray, y: np.ndarray, test_size: float = 0.2, random_state: int = 42\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression decoder and returns accuracy and weights.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix (n_samples, n_features).\n",
    "        y (np.ndarray): Labels (n_samples,).\n",
    "        test_size (float): Fraction of data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (accuracy, weights) where accuracy is a float and weights is (n_features,)\n",
    "\n",
    "    Examples:\n",
    "        >>> np.random.seed(42)\n",
    "        >>> # Create separable data\n",
    "        >>> X = np.vstack([np.random.randn(10, 2) + [2, 0], np.random.randn(10, 2) + [-2, 0]])\n",
    "        >>> y = np.array([0]*10 + [1]*10)\n",
    "        >>> acc, weights = train_and_evaluate_decoder(X, y)\n",
    "        >>> acc >= 0.5  # Should be better than chance\n",
    "        True\n",
    "        >>> len(weights)\n",
    "        2\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Split data using train_test_split\n",
    "    # 2. Create and train LogisticRegression model\n",
    "    # 3. Predict on test set and calculate accuracy\n",
    "    # 4. Return accuracy and model weights (clf.coef_[0])\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementations\n",
    "doctest.run_docstring_examples(prepare_data_for_sklearn, globals())\n",
    "doctest.run_docstring_examples(train_and_evaluate_decoder, globals())\n",
    "\n",
    "\n",
    "# Use the functions\n",
    "X, y = prepare_data_for_sklearn(data, duration)\n",
    "accuracy, weights = train_and_evaluate_decoder(X, y)\n",
    "\n",
    "print(f\"\\nLogistic Regression Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot weights\n",
    "fig = px.bar(\n",
    "    x=range(n_neurons), y=weights, labels={\"x\": \"Neuron\", \"y\": \"Weight\"}, title=\"Decoder Weights\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 9. Individual vs. Population Decoding\n",
    "\n",
    "A key question in neuroscience is whether information is distributed across a population or encoded by specific single neurons. Let's compare the decoding performance of the entire population against the performance of individual neurons.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Iterate through each neuron.\n",
    "2. Train a decoder using **only** that neuron's activity.\n",
    "3. Evaluate its accuracy.\n",
    "4. Compare the best single neuron to the whole population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### ðŸ§  Let's think about it!\n",
    "\n",
    "Look at the weights plot above.\n",
    "\n",
    "1.  Which neurons have positive weights? Which have negative weights?\n",
    "2.  Does this match how we generated the data? (Recall: Stimulus A had higher rates for neurons 0-9, Stimulus B for 10-19).\n",
    "3.  Why might Logistic Regression be better (or worse) than our simple template matcher?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_accuracies = []\n",
    "\n",
    "for neuron_idx in range(n_neurons):\n",
    "    # 1. Calculate Templates for this neuron\n",
    "    # Extract firing rates for this neuron only\n",
    "    rates_A_neuron = [get_population_vector(trial, duration)[neuron_idx] for trial in train_A]\n",
    "    rates_B_neuron = [get_population_vector(trial, duration)[neuron_idx] for trial in train_B]\n",
    "\n",
    "    mean_A = np.mean(rates_A_neuron)\n",
    "    mean_B = np.mean(rates_B_neuron)\n",
    "\n",
    "    # 2. Decode Test Trials\n",
    "    correct_neuron = 0\n",
    "    total_neuron = 0\n",
    "\n",
    "    for trial in test_A:\n",
    "        rate = get_population_vector(trial, duration)[neuron_idx]\n",
    "        if abs(rate - mean_A) < abs(rate - mean_B):\n",
    "            correct_neuron += 1\n",
    "        total_neuron += 1\n",
    "\n",
    "    for trial in test_B:\n",
    "        rate = get_population_vector(trial, duration)[neuron_idx]\n",
    "        if abs(rate - mean_B) < abs(rate - mean_A):\n",
    "            correct_neuron += 1\n",
    "        total_neuron += 1\n",
    "\n",
    "    individual_accuracies.append(correct_neuron / total_neuron * 100)\n",
    "\n",
    "# Plot results\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=list(range(n_neurons)), y=individual_accuracies, name=\"Individual Neurons\"))\n",
    "\n",
    "# Add population accuracy line\n",
    "population_acc = correct / total * 100\n",
    "fig.add_hline(\n",
    "    y=population_acc, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Population Accuracy\"\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Individual vs. Population Decoding Accuracy\",\n",
    "    xaxis_title=\"Neuron Index\",\n",
    "    yaxis_title=\"Accuracy (%)\",\n",
    "    yaxis_range=[0, 105],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 10. Advanced: NeuroAI Representations\n",
    "\n",
    "We have analyzed spikes in various ways. In modern NeuroAI, we often map these biological properties to concepts in Artificial Neural Networks (ANNs).\n",
    "\n",
    "Let's explore these connections explicitly.\n",
    "\n",
    "### 10.1 Rate Coding & ReLU\n",
    "\n",
    "The **firing rate** ($r$) is the most common abstraction. In ANNs, the **ReLU** (Rectified Linear Unit) activation function $f(x) = \\max(0, x)$ mimics the threshold-linear behavior of neurons: below a threshold, they are silent; above it, their rate increases linearly with input current.\n",
    "\n",
    "Let's simulate a neuron's response to increasing input drive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.linspace(-5, 10, 20)\n",
    "rates = []\n",
    "for inp in inputs:\n",
    "    # Simulate a neuron with a threshold and gain\n",
    "    # r = max(0, input)\n",
    "    true_rate = max(0, inp * 5)  # Gain of 5\n",
    "    # Generate spikes\n",
    "    spikes = generate_spike_train(true_rate, duration=1.0)\n",
    "    # Measure rate\n",
    "    measured_rate = len(spikes) / 1.0\n",
    "    rates.append(measured_rate)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=inputs,\n",
    "    y=rates,\n",
    "    labels={\"x\": \"Input Drive\", \"y\": \"Measured Firing Rate (Hz)\"},\n",
    "    title=\"f-I Curve: The Biological ReLU\",\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=inputs, y=[max(0, x * 5) for x in inputs], mode=\"lines\", name=\"Ideal ReLU\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 10.2 Temporal Coding (Latency)\n",
    "\n",
    "Spiking Neural Networks (SNNs) often use the **time of the first spike** to encode information. Stronger inputs cause a neuron to reach its threshold faster, resulting in a shorter latency. This allows the brain to make decisions very quickly, even before a full rate can be estimated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.linspace(1, 20, 20)  # Positive inputs\n",
    "latencies = []\n",
    "for inp in inputs:\n",
    "    rate = inp * 5\n",
    "    spikes = generate_spike_train(rate, duration=1.0)\n",
    "    if len(spikes) > 0:\n",
    "        latencies.append(spikes[0])\n",
    "    else:\n",
    "        latencies.append(1.0)  # Max duration if no spike\n",
    "\n",
    "fig = px.line(\n",
    "    x=inputs,\n",
    "    y=latencies,\n",
    "    labels={\"x\": \"Input Drive\", \"y\": \"Time to First Spike (s)\"},\n",
    "    title=\"Latency Coding: Stronger Input = Faster Response\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 10.3 Neural Embeddings (PCA)\n",
    "\n",
    "The **population vector** ($P$) can be thought of as an **embedding** in a high-dimensional space. Just like word embeddings in NLP, neural states live on a \"manifold\". We can use dimensionality reduction techniques like **PCA** to visualize this space.\n",
    "\n",
    "Here, we project our 20-dimensional neural activity down to 2 dimensions. Notice how the two stimuli form distinct clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have X (population vectors) and y (labels) from Section 8\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "fig = px.scatter(\n",
    "    x=X_pca[:, 0],\n",
    "    y=X_pca[:, 1],\n",
    "    color=y.astype(str),\n",
    "    labels={\"x\": \"PC1\", \"y\": \"PC2\", \"color\": \"Stimulus\"},\n",
    "    title=\"Neural Manifold: PCA of Population Activity\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 10.4 Sparsity\n",
    "\n",
    "Brains are energy-constrained, so they use **sparse representations** ($a$)â€”only a few neurons are active at any time. This is similar to L1 regularization or sparse autoencoders in AI.\n",
    "\n",
    "We can measure population sparsity using **Hoyer's measure**, which ranges from 0 (all neurons active equally) to 1 (only one neuron active).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate population sparsity for each trial\n",
    "# Hoyer's sparsity measure: (sqrt(n) - L1/L2) / (sqrt(n) - 1)\n",
    "def hoyer_sparsity(x):\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    # Avoid division by zero if vector is all zeros\n",
    "    if np.sum(np.abs(x)) == 0:\n",
    "        return 1.0\n",
    "    l1 = np.linalg.norm(x, 1)\n",
    "    l2 = np.linalg.norm(x, 2)\n",
    "    return (np.sqrt(n) - (l1 / l2)) / (np.sqrt(n) - 1)\n",
    "\n",
    "\n",
    "sparsities = [hoyer_sparsity(trial_vec) for trial_vec in X]\n",
    "mean_sparsity = np.mean(sparsities)\n",
    "\n",
    "print(f\"Mean Population Sparsity (Hoyer's): {mean_sparsity:.4f}\")\n",
    "fig = px.histogram(\n",
    "    x=sparsities, nbins=20, title=\"Distribution of Population Sparsity\", labels={\"x\": \"Sparsity\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 10.5 Criticality & Information Maximization\n",
    "\n",
    "A major hypothesis in NeuroAI is that the brain operates at a **critical point**â€”a phase transition between ordered and chaotic dynamics. Systems at criticality are thought to maximize information processing capabilities (dynamic range, memory, information transmission).\n",
    "\n",
    "Let's test this hypothesis using a simple **Branching Process** model.\n",
    "\n",
    "1.  Start with 1 active neuron.\n",
    "2.  At each time step, an active neuron activates $k$ other neurons in the next step, where $k$ is drawn from a Poisson distribution with mean $\\sigma$ (the **branching ratio**).\n",
    "3.  The \"avalanche\" stops when no neurons are active.\n",
    "4.  We measure the **avalanche size** (total number of spikes).\n",
    "\n",
    "**Hypothesis:** The entropy of the avalanche size distribution (a proxy for information capacity) is maximized when $\\sigma \\approx 1$.\n",
    "\n",
    "### ðŸŽ“ Exercise 7: Criticality and Entropy\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  Implement a function `simulate_avalanche()` to simulate a branching process and return the avalanche size.\n",
    "2.  Run simulations for a range of branching ratios $\\sigma$ (e.g., 0.5 to 1.5).\n",
    "3.  Calculate the entropy of the avalanche sizes for each $\\sigma$.\n",
    "4.  Plot Entropy vs. Branching Ratio. Does it peak at 1?\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- At each step, the number of new active neurons is `np.random.poisson(active_neurons * sigma)`\n",
    "- Use `np.histogram` to compute the probability distribution of avalanche sizes\n",
    "- Remember to filter out zero probabilities before computing entropy\n",
    "- Entropy formula: $H = -\\sum P(x) \\log_2 P(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_avalanche(sigma: float, max_steps: int = 100, max_size: int = 10000) -> int:\n",
    "    \"\"\"\n",
    "    Simulates a branching process and returns the total avalanche size.\n",
    "\n",
    "    A branching process models how activity spreads through a neural network.\n",
    "    Each active neuron probabilistically activates other neurons in the next step.\n",
    "\n",
    "    Args:\n",
    "        sigma (float): Branching ratio - average number of neurons activated by each neuron.\n",
    "            - sigma < 1: Subcritical (activity dies out quickly)\n",
    "            - sigma = 1: Critical (activity persists, power-law distributed sizes)\n",
    "            - sigma > 1: Supercritical (activity explodes)\n",
    "        max_steps (int): Maximum simulation steps to prevent infinite loops.\n",
    "        max_size (int): Maximum avalanche size to prevent memory issues.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of spikes in the avalanche.\n",
    "\n",
    "    Examples:\n",
    "        >>> np.random.seed(42)\n",
    "        >>> simulate_avalanche(0.5)  # Subcritical - small avalanche\n",
    "        1\n",
    "        >>> np.random.seed(123)\n",
    "        >>> simulate_avalanche(0.5)  # Another subcritical example\n",
    "        2\n",
    "        >>> np.random.seed(42)\n",
    "        >>> simulate_avalanche(0.0)  # Zero branching = just initial spike\n",
    "        1\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Start with 1 active neuron, total_size = 1\n",
    "    # 2. Loop until no neurons are active (or max_steps/max_size reached):\n",
    "    #    - next_active = np.random.poisson(active_neurons * sigma)\n",
    "    #    - total_size += next_active\n",
    "    #    - active_neurons = next_active\n",
    "    # 3. Return total_size\n",
    "    ################################\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "doctest.run_docstring_examples(simulate_avalanche, globals())\n",
    "\n",
    "# Simulation Parameters\n",
    "sigmas = np.linspace(0.5, 1.5, 11)  # 11 values from subcritical to supercritical\n",
    "n_trials_avalanche = 500  # Number of avalanche simulations per sigma\n",
    "entropies = []\n",
    "\n",
    "################################\n",
    "# YOUR CODE HERE\n",
    "# For each sigma in sigmas:\n",
    "#   1. Run n_trials_avalanche simulations\n",
    "#   2. Collect avalanche sizes\n",
    "#   3. Compute probability distribution using np.histogram\n",
    "#   4. Calculate entropy of the distribution\n",
    "#   5. Append to entropies list\n",
    "################################\n",
    "\n",
    "# Plot (uncomment after implementing)\n",
    "# fig = px.line(x=sigmas, y=entropies, labels={\"x\": \"Branching Ratio (Ïƒ)\", \"y\": \"Entropy (bits)\"})\n",
    "# fig.add_vline(x=1.0, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Critical Point\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this tutorial, we have journeyed from basic spike train statistics to advanced concepts in NeuroAI.\n",
    "\n",
    "- **Variability**: We learned that the brain is noisy (high CV, Fano Factor), but this noise might be a feature, not a bug.\n",
    "- **Information**: We quantified how much information spikes carry using Entropy and Mutual Information.\n",
    "- **Decoding**: We used Machine Learning to \"read the mind\" of our simulated neurons.\n",
    "- **Representations**: We explored how biological features (thresholds, latency, population vectors) map to artificial neural network concepts (ReLU, Embeddings).\n",
    "- **Criticality**: Finally, we saw how operating at the \"edge of chaos\" maximizes information capacity.\n",
    "\n",
    "These tools form the foundation for analyzing both biological neural data and the internal representations of artificial agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 12. Further Reading\n",
    "\n",
    "### Criticality in the Brain\n",
    "\n",
    "In Section 10.5, we demonstrated that systems at a **critical point** maximize their information capacity (entropy). This is a robust finding in statistical physics and neuroscience.\n",
    "\n",
    "- **Read more**: [Beggs & Plenz (2003) - Neuronal Avalanches in Neocortical Circuits](https://www.jneurosci.org/content/23/35/11167)\n",
    "- **Review**: [The Critical Brain (2014)](https://www.frontiersin.org/articles/10.3389/fncir.2014.00054/full)\n",
    "\n",
    "### Spiking Neural Networks (SNNs)\n",
    "\n",
    "If you are interested in exploring how population coding is used in Spiking Neural Networks (SNNs) for deep learning, check out this tutorial from the **snntorch** library:\n",
    "\n",
    "- [**Population Coding in Spiking Neural Networks**](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_pop.html)\n",
    "\n",
    "It demonstrates how using a population of neurons to encode input data can improve the performance of SNNs compared to single-neuron encoding.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
