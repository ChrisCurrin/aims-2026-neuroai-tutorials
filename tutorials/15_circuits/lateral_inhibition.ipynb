{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lateral Inhibition: Modeling Edge Detection with Spiking Neural Networks\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand** the biological principle of lateral inhibition and its role in sensory processing\n",
    "2. **Implement** a spiking neural network (SNN) with lateral inhibition using `snntorch`\n",
    "3. **Observe** edge enhancement and the Mach band illusion in neural responses\n",
    "4. **Connect** biological lateral inhibition to convolutional neural networks (CNNs)\n",
    "5. **Explain** the efficient coding hypothesis and how lateral inhibition compresses sensory information\n",
    "6. **Explore** Hopfield networks as an advanced application of recurrent connections\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "**Knowledge:**\n",
    "\n",
    "- Basic Python programming and PyTorch\n",
    "- Understanding of neurons and spiking (see Tutorial 13: STDP Learning)\n",
    "- Familiarity with NumPy and basic plotting\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "This tutorial requires the `snntorch` library. Make sure to install it by running:\n",
    "\n",
    "```bash\n",
    "uv sync --extra snn\n",
    "```\n",
    "\n",
    "This will install `snntorch` and its dependencies.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Lateral inhibition is a fundamental principle in neurobiology where an excited neuron reduces the activity of its neighbors. This mechanism increases the contrast and sharpness of sensory responses, enabling the brain to detect edges and boundaries effectively. In the visual system, this is famously observed in retinal ganglion cells and the Mach band illusion.\n",
    "\n",
    "In this tutorial, we will build a simple network of LIF neurons to model this phenomenon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import snntorch as snn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from neuroai import plotting\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Theory: Lateral Inhibition\n",
    "\n",
    "Lateral inhibition is the capacity of an excited neuron to reduce the activity of its neighbors. Lateral inhibition disables the spreading of action potentials from excited neurons to neighboring neurons in the lateral direction. This creates a contrast in stimulation that allows increased sensory perception.\n",
    "\n",
    "Mathematically, we can model the input current $I_i$ to neuron $i$ as:\n",
    "\n",
    "<!-- prettier-ignore -->\n",
    "$$ I_i = X_i - \\sum_{j \\neq i} w_{ij} Y_j $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $X_i$ is the excitatory input from the receptor (or previous layer).\n",
    "- $Y_j$ is the output (spike) of neighbor neuron $j$.\n",
    "- $w_{ij}$ is the inhibitory weight from neuron $j$ to neuron $i$.\n",
    "\n",
    "### The \"Mexican Hat\" Interaction\n",
    "\n",
    "Commonly, the interaction profile is modeled as a \"Mexican Hat\" function (or Difference of Gaussians):\n",
    "\n",
    "1.  **Short-range excitation**: Neurons might excite their immediate neighbors (though in our simple model, we only have self-excitation/input).\n",
    "2.  **Medium-range inhibition**: Neurons inhibit their neighbors.\n",
    "3.  **Long-range silence**: Neurons far away do not interact.\n",
    "\n",
    "In our simplified 1D model, we will implement **local inhibition**, where a neuron inhibits only its immediate left and right neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simulation parameters\n",
    "num_steps = 100\n",
    "num_neurons = 30\n",
    "\n",
    "# Create a 1D edge input (step function)\n",
    "# Light on the left (high intensity), Dark on the right (low intensity)\n",
    "input_signal = torch.zeros(num_steps, num_neurons)\n",
    "input_signal[:, : num_neurons // 2] = 0.8  # High intensity\n",
    "input_signal[:, num_neurons // 2 :] = 0.2  # Low intensity\n",
    "\n",
    "# Plot the input profile (spatial)\n",
    "plotting.plot_input_profile(input_signal, num_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Building the Network\n",
    "\n",
    "We will construct a Spiking Neural Network where neurons are arranged in a 1D line.\n",
    "Each neuron receives:\n",
    "\n",
    "1. **Direct Input**: From the sensory signal (identity connection).\n",
    "2. **Lateral Inhibition**: Inhibitory input from its immediate left and right neighbors.\n",
    "\n",
    "We will use `snntorch.Leaky` neurons. The lateral connections will be implemented as a recurrent weight matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateralInhibitionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A Spiking Neural Network with lateral inhibition.\n",
    "    Neurons are arranged in a 1D line and inhibit their immediate neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_neurons, inhibition_strength=0.5):\n",
    "        super().__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "\n",
    "        # Initialize LIF neuron\n",
    "        # beta is the decay rate of the membrane potential\n",
    "        self.lif = snn.Leaky(beta=0.9)\n",
    "\n",
    "        # Define Lateral Inhibition Weights\n",
    "        # We want a matrix where W[i, i-1] = -w and W[i, i+1] = -w\n",
    "        self.w_lat = torch.zeros(num_neurons, num_neurons)\n",
    "\n",
    "        for i in range(num_neurons):\n",
    "            if i > 0:\n",
    "                self.w_lat[i, i - 1] = -inhibition_strength  # Inhibit left neighbor\n",
    "            if i < num_neurons - 1:\n",
    "                self.w_lat[i, i + 1] = -inhibition_strength  # Inhibit right neighbor\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (num_steps, num_neurons)\n",
    "\n",
    "        Returns:\n",
    "            spk_rec (torch.Tensor): Recorded spikes of shape (num_steps, num_neurons)\n",
    "            mem_rec (torch.Tensor): Recorded membrane potentials\n",
    "        \"\"\"\n",
    "        # x shape: (num_steps, num_neurons)\n",
    "        num_steps = x.shape[0]\n",
    "\n",
    "        # Initialize hidden state (membrane potential)\n",
    "        mem = self.lif.init_leaky()\n",
    "\n",
    "        # Record the spike train and membrane potential\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        # Initialize previous spike for recurrence (at t=0, no previous spikes)\n",
    "        spk = torch.zeros(self.num_neurons)\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Current input is External Input + Lateral Inhibition\n",
    "            # x[step] is shape (num_neurons,)\n",
    "\n",
    "            # Lateral input: W_lat @ spk_prev\n",
    "            # If neighbor spiked last step, I get inhibited this step\n",
    "            lateral_input = torch.matmul(self.w_lat, spk)\n",
    "\n",
    "            # Total current\n",
    "            current_input = x[step] + lateral_input\n",
    "\n",
    "            # Run LIF neuron\n",
    "            spk, mem = self.lif(current_input, mem)\n",
    "\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "\n",
    "# Instantiate the network\n",
    "# We use a relatively high inhibition strength to see the effect clearly\n",
    "net = LateralInhibitionNet(num_neurons=num_neurons, inhibition_strength=5.0)\n",
    "\n",
    "# Visualize the weight matrix\n",
    "plotting.plot_weight_matrix(net.w_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Simulation and Results\n",
    "\n",
    "Now we run the simulation. We will record the spikes and calculate the firing rate for each neuron.\n",
    "We expect to see **edge enhancement**:\n",
    "\n",
    "- The neuron on the bright side of the edge should have a higher firing rate than the interior bright neurons (because it receives less inhibition from the dark side).\n",
    "- The neuron on the dark side of the edge should have a lower firing rate than the interior dark neurons (because it receives more inhibition from the bright side).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation\n",
    "spk_rec, mem_rec = net(input_signal)\n",
    "\n",
    "# Calculate firing rate (spikes per step)\n",
    "firing_rate = spk_rec.float().mean(dim=0)\n",
    "\n",
    "# --- Plotting with Plotly (Dual Axis) ---\n",
    "plotting.plot_simulation_results(input_signal, firing_rate, num_neurons)\n",
    "\n",
    "# --- Raster Plot ---\n",
    "plotting.plot_raster(spk_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### ðŸ§  Let's think about it!\n",
    "\n",
    "Look at the \"Output Firing Rate\" graph above.\n",
    "\n",
    "1.  **Bright Side (Left)**: Why is the firing rate of the neuron just before the edge (index 14) _higher_ than the neurons further left (e.g., index 10)?\n",
    "2.  **Dark Side (Right)**: Why is the firing rate of the neuron just after the edge (index 15) _lower_ than the neurons further right (e.g., index 20)?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal explanation</summary>\n",
    "\n",
    "1.  **Bright Side Enhancement**: The neuron at index 14 is stimulated by bright light. Its left neighbor (13) is also bright (high inhibition), but its right neighbor (15) is dark (low inhibition). Therefore, it receives _less total inhibition_ than the interior bright neurons (who are surrounded by bright neighbors on both sides). Less inhibition = higher firing rate.\n",
    "2.  **Dark Side Suppression**: The neuron at index 15 is stimulated by dim light. Its right neighbor (16) is dim (low inhibition), but its left neighbor (14) is bright (high inhibition). Therefore, it receives _more total inhibition_ than the interior dark neurons. More inhibition = lower firing rate.\n",
    "\n",
    "This \"push-pull\" effect exaggerates the difference at the boundary, making the edge \"pop\" out.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. The Link to Deep Learning: Convolutions\n",
    "\n",
    "You might have noticed that the operation performed by our lateral inhibition network looks suspiciously similar to a fundamental operation in Deep Learning: **Convolution**.\n",
    "\n",
    "In a Convolutional Neural Network (CNN), a \"kernel\" or \"filter\" slides across the input.\n",
    "If we define a kernel $K = [-w, 1, -w]$, and convolve it with our input $X$, we get:\n",
    "\n",
    "<!-- prettier-ignore -->\n",
    "$ (X * K)_i = -w \\cdot X_{i-1} + 1 \\cdot X_i - w \\cdot X_{i+1} $\n",
    "\n",
    "This is exactly what our lateral inhibition circuit is doing!\n",
    "\n",
    "- The **center** of the kernel ($1$) represents the direct excitatory input.\n",
    "- The **surround** of the kernel ($-w$) represents the lateral inhibitory input.\n",
    "\n",
    "In NeuroAI, this specific filter is often called a **Difference of Gaussians (DoG)** or a **Laplacian** filter. It is a standard edge-detection filter in computer vision.\n",
    "\n",
    "### ðŸ§ª Experiment: Comparing SNN to CNN\n",
    "\n",
    "Let's verify this by using a standard PyTorch `Conv1d` layer with fixed weights to replicate the behavior of our biological SNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Conv1d layer\n",
    "# 1 input channel, 1 output channel, kernel size 3, padding 1 (to keep same size)\n",
    "conv_layer = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "# Manually set the weights to mimic our lateral inhibition\n",
    "# Weights: [Left Neighbor, Self, Right Neighbor]\n",
    "# Note: In our SNN, 'Self' was the direct input (weight 1.0) and neighbors were inhibitory.\n",
    "# We need to normalize or scale this to match the firing rate scale, but the shape should be identical.\n",
    "inhibition_w = 0.5\n",
    "conv_weights = torch.tensor([[-inhibition_w, 1.0, -inhibition_w]])\n",
    "conv_layer.weight.data = conv_weights.view(1, 1, 3)\n",
    "\n",
    "# Prepare input for Conv1d (Batch, Channel, Length)\n",
    "input_tensor = input_signal[0].unsqueeze(0).unsqueeze(0)  # Take one time step\n",
    "\n",
    "# Run Convolution\n",
    "with torch.no_grad():\n",
    "    conv_output = conv_layer(input_tensor).squeeze()\n",
    "\n",
    "# Plot comparison\n",
    "plotting.plot_cnn_comparison(firing_rate, conv_output, num_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### ðŸ§  Let's think about it!\n",
    "\n",
    "If a simple Convolutional Layer can do the same thing as our complex Spiking Neural Network, why use spikes?\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal</summary>\n",
    "\n",
    "1.  **Energy Efficiency**: In biology (and neuromorphic hardware), spikes are sparse. If there is no edge (uniform input), the lateral inhibition might silence the neurons completely. No spikes = no energy consumed. A standard CNN consumes energy for every multiplication, regardless of the output.\n",
    "2.  **Time**: SNNs operate in time. The \"edge detection\" can happen very quickly (first spike latency), or integrate over time to improve signal-to-noise ratio.\n",
    "3.  **Plasticity**: In biology, these weights aren't just fixed; they can adapt locally using rules like STDP (Spike-Timing-Dependent Plasticity).\n",
    "</details>\n",
    "\n",
    "### Efficient Coding Hypothesis\n",
    "\n",
    "This tutorial demonstrates a core principle of the **Efficient Coding Hypothesis** (Horace Barlow, 1961).\n",
    "The goal of the sensory system is to represent the environment as efficiently as possible.\n",
    "\n",
    "- Natural scenes have high **redundancy** (neighboring pixels are usually similar).\n",
    "- Transmitting the raw pixel values is wasteful.\n",
    "- Transmitting only the **changes** (edges) removes this redundancy and compresses the information.\n",
    "\n",
    "Lateral inhibition is the brain's built-in compression algorithm!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 5. ðŸŽ“ Exercises\n",
    "\n",
    "### Exercise 1: Wider Inhibition Kernel\n",
    "\n",
    "**Task**\n",
    "\n",
    "The current model only inhibits the immediate neighbors (distance = 1).\n",
    "Modify the `LateralInhibitionNet` class (or create a new one) to implement a wider inhibition kernel.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1.  Create a new class `WideInhibitionNet`.\n",
    "2.  In `__init__`, set up the weights such that:\n",
    "    - Neighbors at distance 1 (i-1, i+1) have inhibition strength `w1`.\n",
    "    - Neighbors at distance 2 (i-2, i+2) have inhibition strength `w2`.\n",
    "3.  Run the simulation with `w1 = 5.0` and `w2 = 2.0`.\n",
    "4.  Plot the results and compare with the previous simple model.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- You will need to add checks in your loop to ensure you don't access indices outside `[0, num_neurons-1]`.\n",
    "- For example, `if i > 1: self.w_lat[i, i-2] = -w2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "class WideInhibitionNet(nn.Module):\n",
    "    def __init__(self, num_neurons, w1=5.0, w2=2.0):\n",
    "        super().__init__()\n",
    "        self.num_neurons = num_neurons\n",
    "        self.lif = snn.Leaky(beta=0.9)\n",
    "        self.w_lat = torch.zeros(num_neurons, num_neurons)\n",
    "\n",
    "        # TODO: Implement the weight initialization loop\n",
    "        for i in range(num_neurons):\n",
    "            pass  # Replace with your code\n",
    "\n",
    "    def forward(self, x):\n",
    "        # You can copy the forward method from the previous class\n",
    "        # or inherit and reuse it\n",
    "        pass\n",
    "\n",
    "\n",
    "# Instantiate and run\n",
    "# wide_net = WideInhibitionNet(...)\n",
    "# spk_wide, _ = wide_net(input_signal)\n",
    "# ... Plotting code ...\n",
    "\n",
    "\n",
    "# Test the solution\n",
    "def test_wide_inhibition():\n",
    "    net = WideInhibitionNet(num_neurons=10, w1=0.5, w2=0.2)\n",
    "    w = net.w_lat\n",
    "    assert w[2, 1] == -0.5, f\"Weight at distance 1 should be -0.5, got {w[2, 1]}\"\n",
    "    assert w[2, 3] == -0.5, f\"Weight at distance 1 should be -0.5, got {w[2, 3]}\"\n",
    "    assert w[2, 0] == -0.2, f\"Weight at distance 2 should be -0.2, got {w[2, 0]}\"\n",
    "    assert w[2, 4] == -0.2, f\"Weight at distance 2 should be -0.2, got {w[2, 4]}\"\n",
    "    print(\"âœ… Exercise 1 Solution Passed!\")\n",
    "\n",
    "\n",
    "test_wide_inhibition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results of Exercise 1\n",
    "\n",
    "# Instantiate the wide inhibition network\n",
    "wide_net = WideInhibitionNet(num_neurons=num_neurons, w1=5.0, w2=2.0)\n",
    "\n",
    "# Visualize the weight matrix to see the wider inhibition pattern\n",
    "plotting.plot_weight_matrix(wide_net.w_lat, title=\"Wide Inhibition Weight Matrix (Distance 1 & 2)\")\n",
    "\n",
    "# Run simulation with wide inhibition network\n",
    "spk_wide, _ = wide_net(input_signal)\n",
    "firing_rate_wide = spk_wide.float().mean(dim=0)\n",
    "\n",
    "# Compare firing rates: Original vs Wide Inhibition\n",
    "fig = go.Figure()\n",
    "\n",
    "# Original network (narrow inhibition)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_neurons),\n",
    "        y=firing_rate.detach().numpy(),\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Narrow Inhibition (distance=1)\",\n",
    "        line=dict(color=\"blue\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Wide inhibition network\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=np.arange(num_neurons),\n",
    "        y=firing_rate_wide.detach().numpy(),\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Wide Inhibition (distance=1,2)\",\n",
    "        line=dict(color=\"red\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add vertical line at edge boundary\n",
    "fig.add_vline(x=num_neurons // 2 - 0.5, line_dash=\"dash\", line_color=\"gray\", opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparison: Narrow vs Wide Lateral Inhibition\",\n",
    "    xaxis_title=\"Neuron Index\",\n",
    "    yaxis_title=\"Firing Rate\",\n",
    "    hovermode=\"x unified\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Also plot the raster plot for wide inhibition\n",
    "plotting.plot_raster(spk_wide)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Exercise 2: The Effect of Inhibition Strength\n",
    "\n",
    "**Task**\n",
    "\n",
    "Investigate how the strength of lateral inhibition affects the edge detection capability.\n",
    "Run the simulation with the original `LateralInhibitionNet` using three different inhibition strengths:\n",
    "\n",
    "1.  `inhibition_strength = 0.0` (No inhibition)\n",
    "2.  `inhibition_strength = 2.0` (Moderate inhibition)\n",
    "3.  `inhibition_strength = 10.0` (Strong inhibition)\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- What happens to the firing rates when inhibition is zero?\n",
    "- Does the \"Mach band\" effect appear in all cases?\n",
    "- What happens when inhibition is very strong? Does it suppress the signal too much?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 2 here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Exercise 3: Robustness to Noise\n",
    "\n",
    "**Task**\n",
    "\n",
    "Real-world sensory signals are rarely clean; they are often corrupted by noise.\n",
    "\n",
    "1.  Create a noisy version of the input signal by adding Gaussian noise.\n",
    "    ```python\n",
    "    noise = torch.randn_like(input_signal) * 0.1\n",
    "    noisy_input = input_signal + noise\n",
    "    noisy_input = torch.clamp(noisy_input, 0, 1) # Keep values between 0 and 1\n",
    "    ```\n",
    "2.  Feed this `noisy_input` into the `LateralInhibitionNet` (use `inhibition_strength=5.0`).\n",
    "3.  Plot the input intensity vs. output firing rate.\n",
    "\n",
    "**Question:**\n",
    "\n",
    "- Does the network still detect the edge despite the noise?\n",
    "- Does lateral inhibition amplify or suppress the noise in the uniform regions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Exercise 3 here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 6. Advanced Topic: From Inhibition to Memory (Hopfield Networks)\n",
    "\n",
    "We have seen how **fixed inhibitory** recurrent connections can perform useful computations like edge detection.\n",
    "But what happens if we have **learnable excitatory** recurrent connections?\n",
    "\n",
    "This leads us to the concept of **Attractor Networks** and **Associative Memory**. The most famous example is the **Hopfield Network** (John Hopfield, 1982).\n",
    "\n",
    "### Theory: Energy Landscapes and Hebbian Learning\n",
    "\n",
    "A Hopfield network is a fully connected recurrent network (with symmetric weights) that acts as a content-addressable memory.\n",
    "Instead of mapping input $X \\to Y$, it maps an initial state $S_{init} \\to S_{stable}$, where $S_{stable}$ is a stored memory pattern.\n",
    "\n",
    "The network minimizes an \"Energy Function\":\n",
    "\n",
    "<!-- prettier-ignore -->\n",
    "$$ E = -\\frac{1}{2} \\sum_{i,j} w_{ij} s_i s_j $$\n",
    "\n",
    "If we update neurons asynchronously to align with their local field, the energy $E$ is guaranteed to decrease (or stay same) until it reaches a local minimum (attractor).\n",
    "\n",
    "**Hebbian Learning Rule**:\n",
    "To store patterns $\\xi^{\\mu}$, we set the weights proportional to the correlation between neurons:\n",
    "\n",
    "<!-- prettier-ignore -->\n",
    "$$ w_{ij} = \\frac{1}{N} \\sum_{\\mu=1}^{P} \\xi_i^{\\mu} \\xi_j^{\\mu} $$\n",
    "\"Neurons that fire together, wire together.\"\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's implement a simple binary Hopfield network to store and recover images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopfieldNetwork:\n",
    "    def __init__(self, num_neurons):\n",
    "        self.num_neurons = num_neurons\n",
    "        self.weights = torch.zeros(num_neurons, num_neurons)\n",
    "\n",
    "    def train(self, patterns):\n",
    "        \"\"\"\n",
    "        Train using Hebbian rule.\n",
    "        patterns: (P, N) tensor of -1 and +1\n",
    "        \"\"\"\n",
    "        P, N = patterns.shape\n",
    "        # W = (1/N) * X.T @ X\n",
    "        self.weights = (1.0 / N) * torch.matmul(patterns.T, patterns)\n",
    "\n",
    "        # Remove self-connections (diagonal = 0)\n",
    "        self.weights.fill_diagonal_(0)\n",
    "\n",
    "    def predict(self, state, steps=10):\n",
    "        \"\"\"\n",
    "        Asynchronous update dynamics.\n",
    "        \"\"\"\n",
    "        state = state.clone()\n",
    "        for _ in range(steps):\n",
    "            # Compute local field\n",
    "            # h = W @ s\n",
    "            h = torch.matmul(self.weights, state)\n",
    "\n",
    "            # Update rule: s = sign(h)\n",
    "            # Note: In true async, we update one by one.\n",
    "            # Here we do synchronous for simplicity (Little model),\n",
    "            # or we can loop through indices.\n",
    "            state = torch.sign(h)\n",
    "\n",
    "            # Handle zero case (keep previous state)\n",
    "            state[h == 0] = state[h == 0]\n",
    "\n",
    "        return state\n",
    "\n",
    "\n",
    "# Create simple patterns (e.g., 5x5 images flattened to 25 neurons)\n",
    "N_sqrt = 10\n",
    "N = N_sqrt * N_sqrt\n",
    "\n",
    "# Pattern 1: A Cross\n",
    "p1 = -torch.ones(N_sqrt, N_sqrt)\n",
    "p1[N_sqrt // 2, :] = 1\n",
    "p1[:, N_sqrt // 2] = 1\n",
    "p1 = p1.flatten()\n",
    "\n",
    "# Pattern 2: A Square\n",
    "p2 = -torch.ones(N_sqrt, N_sqrt)\n",
    "p2[2:-2, 2:-2] = 1\n",
    "p2 = p2.flatten()\n",
    "\n",
    "patterns = torch.stack([p1, p2])\n",
    "\n",
    "# Train\n",
    "hopfield = HopfieldNetwork(N)\n",
    "hopfield.train(patterns)\n",
    "\n",
    "# Visualize Patterns\n",
    "plotting.plot_hopfield_patterns(p1, p2, N_sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Visualizing the Hopfield Connectivity\n",
    "\n",
    "The \"knowledge\" of the Hopfield network is stored in its weight matrix.\n",
    "Unlike the lateral inhibition network where weights were fixed and local (only neighbors), here the weights are **global** (fully connected) and **learned** from the data.\n",
    "\n",
    "Let's visualize the weight matrix. You might notice some structure that reflects the stored patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights\n",
    "plotting.plot_weight_matrix(\n",
    "    hopfield.weights, title=\"Hopfield Network Weight Matrix (Hebbian Learning)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Visualizing the Network Graph\n",
    "\n",
    "To better understand the \"all-to-all\" connectivity, let's visualize a small Hopfield network as a graph.\n",
    "\n",
    "- **Nodes** represent neurons.\n",
    "- **Edges** represent the learned weights between them.\n",
    "- **Red edges** are inhibitory (negative weights).\n",
    "- **Blue edges** are excitatory (positive weights).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Network Graph\n",
    "plotting.plot_hopfield_topology(HopfieldNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Associative Memory Demo\n",
    "\n",
    "Now we will:\n",
    "\n",
    "1.  Take a stored pattern.\n",
    "2.  Corrupt it with noise (flip some bits).\n",
    "3.  Let the network evolve.\n",
    "4.  See if it recovers the original pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrupt Pattern 1\n",
    "noise_level = 0.3\n",
    "corrupted_p1 = p1.clone()\n",
    "mask = torch.rand(N) < noise_level\n",
    "corrupted_p1[mask] *= -1  # Flip bits\n",
    "\n",
    "# Recover\n",
    "recovered_p1 = hopfield.predict(corrupted_p1, steps=5)\n",
    "\n",
    "# Plot\n",
    "plotting.plot_hopfield_recall(p1, corrupted_p1, recovered_p1, N_sqrt, noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### ðŸ“š Further Reading on Hopfield Networks\n",
    "\n",
    "- **Original Paper**: Hopfield, J. J. (1982). \"Neural networks and physical systems with emergent collective computational abilities\". _PNAS_.\n",
    "- **Modern Hopfield Networks**: Recent work (e.g., \"Dense Associative Memory\" by Krotov & Hopfield, 2016) has shown that changing the energy function can drastically increase storage capacity, linking Hopfield Networks to **Transformers** (Attention mechanisms).\n",
    "- **Scholarpedia**: [Hopfield Network](http://www.scholarpedia.org/article/Hopfield_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the power of recurrent connections in neural networks:\n",
    "\n",
    "1.  **Lateral Inhibition**: We saw how fixed inhibitory connections create **edge detection** and contrast enhancement (Mach bands), a principle used in the retina and modeled by Convolutions in Deep Learning.\n",
    "2.  **Hopfield Networks**: We saw how learnable excitatory connections create **associative memory**, allowing networks to store and recover patterns from noise.\n",
    "\n",
    "These two examples demonstrate how the _structure_ of connections (topology) determines the _function_ of the network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
