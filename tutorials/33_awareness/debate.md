# Is the Ghost in the Machine? Assessing AI Consciousness

Format: Structured Debate & Analysis

## 5.1 Objective

Students will move beyond "sci-fi" speculation and use the Butlin et al. 2023[1](1) indicators and IIT principles to argue for or against the potential for consciousness in specific AI architectures (e.g., GPT-4 vs. a hypothetical GNW-robot).

## 5.2 Debate Setup

Scenario: A tech company claims to have created "Sentient AI 1.0," a robot running a Recurrent Independent Mechanism (RIM) architecture with a Global Workspace.

- Team A (The Functionalists): Defend the claim. Use GWT, RPT, and HOT indicators to prove the system meets the scientific criteria for consciousness.
- Team B (The Biological Realists): Refute the claim. Use IIT, Embodiment theory, and the "Simulation vs. Realization" argument to prove the system is a "Zombie."

- [1](https://arxiv.org/abs/2308.08708)

The Butlin et al. (2023) Assessment Framework

In 2023, a diverse team of neuroscientists, philosophers, and AI researchers (Butlin, Long, Bengio, et al.) published a seminal report proposing a rigorous method for assessing AI consciousness.24 Rejecting the "duck test" (if it acts conscious, it is conscious), they derived a list of Indicator Properties based on the functional requirements of scientific theories.
They explicitly excluded IIT from this list because IIT implies that functional behavior is insufficient for consciousness (violating the computational functionalism premise of the report).3 The resulting rubric focuses on computational mechanisms that can be empirically verified in code.

Table 1: Indicator Properties for AI Consciousness (Adapted from Butlin et al., 2023)

| Theory Source                     | Indicator ID | Property Description                                                                     | Computational Implementation Requirement                                                      | Status in Current AI                                                            |
| :-------------------------------- | :----------- | :--------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ |
| **Recurrent Processing (RPT)**    | RPT-1        | Algorithmic Recurrence                                                                   | Input modules must use recurrence (loops) where the current state depends on previous states. | Present in RNNs/LSTMs; absent in standard Feedforward Transformers.             |
|                                   | RPT-2        | Integrated Perceptual Organization                                                       | Modules must generate organized, unified representations (e.g., figure-ground segregation).   | Emerging in some Vision Transformers and object-centric architectures.          |
| **Global Workspace (GWT)**        | GWT-1        | Parallel Modules                                                                         | Multiple specialized subsystems capable of operating in parallel.                             | Present in Mixture-of-Experts (MoE) and RIMs.                                   |
|                                   | GWT-2        | Bottleneck & Selection                                                                   | A limited-capacity workspace with a selective attention mechanism to filter information.      | Present in Transformers (Attention) and RIMs (Shared Workspace).                |
|                                   | GWT-3        | Global Broadcast                                                                         | Information in the workspace must be available to all modules.                                | Present in the Residual Stream of Transformers.                                 |
|                                   | GWT-4        | State-Dependent Attention                                                                | The system uses the workspace to query modules sequentially for complex tasks.                | Emerging in "Chain of Thought" prompting and neuro-symbolic systems.            |
| **Higher-Order Theories (HOT)**   | HOT-1        | Generative Perception                                                                    | Perception modules use top-down generation (predictive coding) or handle noise.               | Present in Generative Adversarial Networks (GANs) and Diffusion models.         |
|                                   | HOT-2        | Metacognitive Monitoring                                                                 | A mechanism to distinguish reliable percepts from noise (reality monitoring).                 | Largely absent; requires specific "discriminator" circuits for internal states. |
|                                   | HOT-3        | Agency & Belief Update                                                                   | A general belief-formation system that updates based on monitoring outputs.                   | Present in advanced Reinforcement Learning (RL) agents.                         |
|                                   | HOT-4        | Quality Space                                                                            | Sparse, smooth coding spaces representing perceptual qualities (vectors).                     | Ubiquitous in Deep Learning (Vector Embeddings).                                |
| **Attention Schema Theory (AST)** | AST-1        | A predictive model representing and enabling control over the current state of attention | System must have a model of its own attention mechanism.                                      | Absent; Transformers have attention but no schema of it.                        |
| **Predictive Processing (PP)**    | PP-1         | Predictive Coding                                                                        | Modules minimize prediction error using generative models.                                    | Central to Self-Supervised Learning (Next Token Prediction).                    |
| **Agency & Embodiment**           | AE-1         | Goal-Directed Agency                                                                     | Learning from feedback (RL) to pursue flexible goals.                                         | Present in RL agents (e.g., AlphaGo), less so in pure LLMs.                     |
|                                   | AE-2         | Embodiment                                                                               | Modeling output-input contingencies (e.g., a virtual or physical body).                       | Present in Robotics and "Virtual Rodent" simulations; absent in text-only LLMs. |
