{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neuron and The Perceptron\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- Implement an artificial neuron (perceptron) and understand its computational properties\n",
    "- Implement a biologically-inspired neuron (Leaky Integrate-and-Fire) with membrane dynamics\n",
    "- Model synaptic input using conductance-based synapses\n",
    "- Implement short-term plasticity (synaptic depression) with finite vesicle pools\n",
    "- Compare input, processing, and output between artificial and biological neurons\n",
    "\n",
    "## ðŸ“š Prerequisites\n",
    "\n",
    "- **Conceptual**: Complete Tutorial 01 (What is NeuroAI?) for background\n",
    "- **Technical**: Basic Python programming, familiarity with NumPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from neuroai.plotting import (\n",
    "    plot_activation_functions_comparison,\n",
    "    plot_complete_neuron,\n",
    "    plot_fi_curve,\n",
    "    plot_io_comparison,\n",
    "    plot_layer_response,\n",
    "    plot_lif_response,\n",
    "    plot_perceptron_computation,\n",
    "    plot_stp_comparison,\n",
    "    plot_synapse_comparison,\n",
    "    plot_synaptic_response,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Perceptron (Artificial Neuron)\n",
    "\n",
    "The perceptron, introduced by Frank Rosenblatt in 1958, is the simplest artificial neuron. Despite its simplicity, it forms the building block of modern deep learning.\n",
    "\n",
    "### The Perceptron Computation\n",
    "\n",
    "1. **Input**: Receive a vector of inputs $\\mathbf{x} = [x_1, x_2, ..., x_n]$\n",
    "2. **Processing**:\n",
    "   - Compute weighted sum: $z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "   - Apply activation function: $y = \\sigma(z)$\n",
    "3. **Output**: A single scalar value $y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Activation Functions\n",
    "\n",
    "The activation function $\\sigma$ introduces non-linearity. Let's visualize the common ones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Sigmoid activation function.\n",
    "\n",
    "    Squashes any input to the range (0, 1).\n",
    "\n",
    "    Args:\n",
    "        z: Input array\n",
    "\n",
    "    Returns:\n",
    "        Output squeezed between 0 and 1\n",
    "\n",
    "    Examples:\n",
    "        >>> sigmoid(np.array([0.0]))\n",
    "        array([0.5])\n",
    "        >>> sigmoid(np.array([-100, 100])).round(3)\n",
    "        array([0., 1.])\n",
    "        >>> sigmoid(np.array([-1, 0, 1])).round(3)\n",
    "        array([0.269, 0.5  , 0.731])\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def relu(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"ReLU (Rectified Linear Unit) activation function.\n",
    "\n",
    "    Returns max(0, z) for each element.\n",
    "\n",
    "    Args:\n",
    "        z: Input array\n",
    "\n",
    "    Returns:\n",
    "        Array with negative values set to 0\n",
    "\n",
    "    Examples:\n",
    "        >>> relu(np.array([-2, -1, 0, 1, 2]))\n",
    "        array([0, 0, 0, 1, 2])\n",
    "        >>> relu(np.array([0.5]))\n",
    "        array([0.5])\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def tanh_activation(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Hyperbolic tangent activation function.\n",
    "\n",
    "    Squashes input to the range (-1, 1).\n",
    "\n",
    "    Examples:\n",
    "        >>> tanh_activation(np.array([0.0]))\n",
    "        array([0.])\n",
    "        >>> tanh_activation(np.array([-100, 100]))\n",
    "        array([-1.,  1.])\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "# Run doctests\n",
    "# This is how we check our Examples in the docstrings are correct\n",
    "# We will use this pattern to help with verifying our solutions in the tutorials\n",
    "doctest.run_docstring_examples(sigmoid, globals(), name=\"sigmoid\")\n",
    "doctest.run_docstring_examples(relu, globals(), name=\"relu\")\n",
    "doctest.run_docstring_examples(tanh_activation, globals(), name=\"tanh_activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "z = np.linspace(-5, 5, 200)\n",
    "plot_activation_functions_comparison(\n",
    "    z,\n",
    "    sigmoid(z),\n",
    "    relu(z),\n",
    "    tanh_activation(z),\n",
    "    same_range=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementing the Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"A simple perceptron (artificial neuron).\n",
    "\n",
    "    Computes: y = activation(w Â· x + b)\n",
    "\n",
    "    Attributes:\n",
    "        weights: Connection weights for each input\n",
    "        bias: Bias term\n",
    "        activation: Activation function to apply\n",
    "\n",
    "    Examples:\n",
    "        >>> p = Perceptron(n_inputs=2)\n",
    "        >>> p.weights = np.array([1.0, -1.0])\n",
    "        >>> p.bias = 0.0\n",
    "        >>> p.forward(np.array([1.0, 1.0]))  # 1*1 + (-1)*1 + 0 = 0 -> sigmoid(0) = 0.5\n",
    "        0.5\n",
    "        >>> p.forward(np.array([2.0, 0.0]))  # 1*2 + (-1)*0 + 0 = 2 -> sigmoid(2) â‰ˆ 0.88\n",
    "        0.8807970779778823\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs: int, activation=sigmoid):\n",
    "        \"\"\"Initialize perceptron with random weights.\n",
    "\n",
    "        Args:\n",
    "            n_inputs: Number of input connections\n",
    "            activation: Activation function (default: sigmoid)\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.activation = activation\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weights = rng.normal(scale=0.5, size=self.n_inputs)\n",
    "        self.bias = rng.normal(scale=0.1)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> float:\n",
    "        \"\"\"Compute the perceptron output.\n",
    "\n",
    "        Args:\n",
    "            x: Input array of shape (n_inputs,)\n",
    "\n",
    "        Returns:\n",
    "            Single output value after activation\n",
    "        \"\"\"\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        y = self.activation(z)\n",
    "        return float(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the perceptron\n",
    "perceptron = Perceptron(n_inputs=3)\n",
    "test_input = np.array([1.0, 0.5, -0.3])\n",
    "output = perceptron.forward(test_input)\n",
    "\n",
    "print(\"PERCEPTRON\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Input:   {test_input}\")\n",
    "print(f\"Weights: {perceptron.weights.round(3)}\")\n",
    "print(f\"Bias:    {perceptron.bias:.3f}\")\n",
    "print(f\"Output:  {output:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the perceptron computation step-by-step\n",
    "weighted_inputs = perceptron.weights * test_input\n",
    "z = np.sum(weighted_inputs) + perceptron.bias\n",
    "plot_perceptron_computation(weighted_inputs, perceptron.bias, z, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Let's think about it!\n",
    "\n",
    "Notice the key properties of the perceptron:\n",
    "\n",
    "1. **Instantaneous**: Given an input, we immediately get an output. No notion of time.\n",
    "2. **Stateless**: The same input always produces the same output.\n",
    "3. **Rate-coded**: The output is a continuous value, interpreted as a \"firing rate\".\n",
    "4. **Memoryless**: Previous inputs don't affect current output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Population of Perceptrons\n",
    "\n",
    "In real neural networks, neurons work together in **layers**. Implement a function that creates multiple perceptrons processing the same input.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Each perceptron should have its own random weights\n",
    "- Return the outputs of all perceptrons as an array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_layer(inputs: np.ndarray, n_neurons: int = 10) -> np.ndarray:\n",
    "    \"\"\"Simulate a layer of perceptrons processing the same input.\n",
    "\n",
    "    Args:\n",
    "        inputs: Input array of shape (n_features,)\n",
    "        n_neurons: Number of perceptrons in the layer\n",
    "\n",
    "    Returns:\n",
    "        Array of outputs from each perceptron, shape (n_neurons,)\n",
    "\n",
    "    Examples:\n",
    "        >>> outputs = perceptron_layer(np.array([1.0, 0.5]), n_neurons=5)\n",
    "        >>> outputs.shape\n",
    "        (5,)\n",
    "        >>> all(0 <= o <= 1 for o in outputs)  # All outputs in sigmoid range\n",
    "        True\n",
    "        >>> len(set(outputs.round(4))) > 1  # Different neurons give different outputs\n",
    "        True\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function!\n",
    "    # Create n_neurons perceptrons, each processing the input\n",
    "    # Return array of their outputs\n",
    "    raise NotImplementedError(\"Implement perceptron_layer\")\n",
    "\n",
    "\n",
    "doctest.run_docstring_examples(perceptron_layer, globals(), name=\"perceptron_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the layer response\n",
    "layer_outputs = perceptron_layer(np.array([0.8, 0.5, 0.9]), n_neurons=20)\n",
    "plot_layer_response(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Leaky Integrate-and-Fire (LIF) Neuron\n",
    "\n",
    "Real neurons are far more complex. They:\n",
    "\n",
    "- Operate in **continuous time**\n",
    "- Have **membrane dynamics** (voltage that rises and falls)\n",
    "- Communicate via discrete **spikes** (action potentials)\n",
    "- Exhibit **temporal integration** (inputs accumulate over time)\n",
    "\n",
    "### The LIF Equation\n",
    "\n",
    "$$\\tau_m \\frac{dV}{dt} = -(V - V_{rest}) + R \\cdot I(t)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $V$ is the membrane potential (mV)\n",
    "- $V_{rest}$ is the resting potential (~-70 mV)\n",
    "- $\\tau_m$ is the membrane time constant (how quickly voltage decays)\n",
    "- $R$ is the membrane resistance\n",
    "- $I(t)$ is the input current\n",
    "\n",
    "**Spiking**: When $V \\geq V_{threshold}$, the neuron fires and resets to $V_{reset}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement the LIF Voltage Update\n",
    "\n",
    "The LIF equation describes how the membrane potential changes over time:\n",
    "\n",
    "$$\\tau_m \\frac{dV}{dt} = -(V - V_{rest}) + R \\cdot I$$\n",
    "\n",
    "Using **Euler integration**, we can discretize this:\n",
    "\n",
    "$$dV = \\left( -(V - V_{rest}) + R \\cdot I \\right) \\cdot \\frac{dt}{\\tau_m}$$\n",
    "\n",
    "$$V_{new} = V + dV$$\n",
    "\n",
    "**Your task**: Implement a single Euler step of this equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_voltage_step(v: float, v_rest: float, tau_m: float, R: float, I: float, dt: float) -> float:\n",
    "    \"\"\"Implement one Euler step of the LIF membrane equation.\n",
    "\n",
    "    The LIF equation is:\n",
    "        Ï„_m * dV/dt = -(V - V_rest) + R * I\n",
    "\n",
    "    Using Euler integration:\n",
    "        dV = (-(V - V_rest) + R * I) * (dt / Ï„_m)\n",
    "        V_new = V + dV\n",
    "\n",
    "    Args:\n",
    "        v: Current membrane potential (mV)\n",
    "        v_rest: Resting potential (mV)\n",
    "        tau_m: Membrane time constant (ms)\n",
    "        R: Membrane resistance (MOhm)\n",
    "        I: Input current (nA)\n",
    "        dt: Time step (ms)\n",
    "\n",
    "    Returns:\n",
    "        New membrane potential (mV)\n",
    "\n",
    "    Examples:\n",
    "        >>> lif_voltage_step(-70, -70, 20, 10, 0, 0.1)  # At rest, no input\n",
    "        -70.0\n",
    "        >>> lif_voltage_step(-70, -70, 20, 10, 20, 0.1)  # With input current\n",
    "        -69.0\n",
    "        >>> lif_voltage_step(-60, -70, 20, 10, 0, 0.1)  # Above rest, decays back\n",
    "        -60.05\n",
    "    \"\"\"\n",
    "    # TODO: Implement the Euler step!\n",
    "    # 1. Calculate dV using the discretized LIF equation\n",
    "    # 2. Return V + dV\n",
    "    raise NotImplementedError(\"Implement lif_voltage_step\")\n",
    "\n",
    "\n",
    "doctest.run_docstring_examples(lif_voltage_step, globals(), name=\"lif_voltage_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron:\n",
    "    \"\"\"Leaky Integrate-and-Fire neuron model.\n",
    "\n",
    "    A biologically-inspired neuron that integrates input current\n",
    "    over time and fires discrete spikes when threshold is reached.\n",
    "\n",
    "    Attributes:\n",
    "        tau_m: Membrane time constant (ms)\n",
    "        v_rest: Resting potential (mV)\n",
    "        v_reset: Reset potential after spike (mV)\n",
    "        v_threshold: Firing threshold (mV)\n",
    "        resistance: Membrane resistance (MOhm)\n",
    "\n",
    "    Examples:\n",
    "        >>> lif = LIFNeuron(v_rest=-70, v_threshold=-55)\n",
    "        >>> lif.v  # Starts at rest\n",
    "        -70\n",
    "        >>> lif.step(I=0, dt=1.0)  # No input, no spike\n",
    "        False\n",
    "        >>> lif.v  # Still at rest (no input)\n",
    "        -70.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau_m: float = 20.0,\n",
    "        v_rest: float = -70.0,\n",
    "        v_reset: float = -80.0,\n",
    "        v_threshold: float = -55.0,\n",
    "        resistance: float = 10.0,\n",
    "    ):\n",
    "        self.tau_m = tau_m\n",
    "        self.v_rest = v_rest\n",
    "        self.v_reset = v_reset\n",
    "        self.v_threshold = v_threshold\n",
    "        self.resistance = resistance\n",
    "        self.v = v_rest\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset neuron to resting state.\"\"\"\n",
    "        self.v = self.v_rest\n",
    "\n",
    "    def step(self, I: float, dt: float = 0.1) -> bool:\n",
    "        \"\"\"Simulate one time step using Euler integration.\n",
    "\n",
    "        Args:\n",
    "            I: Input current (nA)\n",
    "            dt: Time step size (ms)\n",
    "\n",
    "        Returns:\n",
    "            True if neuron spiked, False otherwise\n",
    "        \"\"\"\n",
    "        dv = (-(self.v - self.v_rest) + self.resistance * I) * (dt / self.tau_m)\n",
    "        self.v += dv\n",
    "\n",
    "        if self.v >= self.v_threshold:\n",
    "            self.v = self.v_reset\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def simulate(self, current: np.ndarray, dt: float = 0.1) -> tuple[np.ndarray, list[float]]:\n",
    "        \"\"\"Simulate the neuron over a current time series.\n",
    "\n",
    "        Args:\n",
    "            current: Input current array (one value per time step)\n",
    "            dt: Time step size (ms)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (membrane_potential_trace, spike_times_in_ms)\n",
    "        \"\"\"\n",
    "        self.reset_state()\n",
    "        v_trace = []\n",
    "        spike_times = []\n",
    "\n",
    "        for t_idx, I in enumerate(current):\n",
    "            spiked = self.step(I, dt)\n",
    "            v_trace.append(self.v)\n",
    "            if spiked:\n",
    "                spike_times.append(t_idx * dt)\n",
    "\n",
    "        return np.array(v_trace), spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LIF neuron with step current\n",
    "lif = LIFNeuron()\n",
    "print(\"LIF NEURON\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Membrane time constant: {lif.tau_m} ms\")\n",
    "print(f\"Resting potential: {lif.v_rest} mV\")\n",
    "print(f\"Threshold: {lif.v_threshold} mV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate LIF dynamics with step current\n",
    "dt = 0.1\n",
    "duration = 200  # ms\n",
    "time = np.arange(0, duration, dt)\n",
    "\n",
    "# Step current: off -> on -> off\n",
    "current = np.zeros_like(time)\n",
    "current[(time >= 20) & (time < 150)] = 2.0  # 2 nA from 20-150 ms\n",
    "\n",
    "# Simulate\n",
    "lif = LIFNeuron()\n",
    "v_trace, spike_times = lif.simulate(current, dt)\n",
    "\n",
    "# Plot\n",
    "plot_lif_response(time, current, v_trace, spike_times, lif.v_threshold, lif.v_rest)\n",
    "\n",
    "print(f\"\\nNumber of spikes: {len(spike_times)}\")\n",
    "print(f\"Firing rate: {len(spike_times) / (duration / 1000):.1f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Let's think about it!\n",
    "\n",
    "Observe the LIF dynamics:\n",
    "\n",
    "1. **Integration**: The voltage ramps up when current is applied (not instant!)\n",
    "2. **Leaky**: The voltage decays back to rest when current stops\n",
    "3. **Spiking**: When threshold is crossed, the neuron fires and resets\n",
    "4. **Refractory**: After reset, it takes time to reach threshold again\n",
    "\n",
    "**Try it**: Change the input current magnitude. How does the firing rate change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement an F-I Curve\n",
    "\n",
    "The **F-I curve** (Frequency vs. Input current) is a fundamental characterization of a neuron. Implement a function that computes firing rate for different input currents.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Simulate the neuron for each current level\n",
    "- Compute firing rate = number of spikes / duration (in seconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fi_curve(currents: np.ndarray, duration: float = 500, dt: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"Compute the F-I curve (firing rate vs input current).\n",
    "\n",
    "    Args:\n",
    "        currents: Array of input current values to test (nA)\n",
    "        duration: Simulation duration (ms)\n",
    "        dt: Time step (ms)\n",
    "\n",
    "    Returns:\n",
    "        Array of firing rates (Hz) for each current\n",
    "\n",
    "    Examples:\n",
    "        >>> rates = compute_fi_curve(np.array([0.0, 1.0, 2.0, 3.0]))\n",
    "        >>> rates.shape\n",
    "        (4,)\n",
    "        >>> float(rates[0])  # No current = no spikes\n",
    "        0.0\n",
    "        >>> bool(rates[1] < rates[2] < rates[3])  # Higher current = higher rate\n",
    "        True\n",
    "        >>> all(r >= 0 for r in rates)  # Rates are non-negative\n",
    "        True\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    raise NotImplementedError(\"Implement compute_fi_curve\")\n",
    "\n",
    "\n",
    "doctest.run_docstring_examples(compute_fi_curve, globals(), name=\"compute_fi_curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot F-I curve\n",
    "currents = np.linspace(0, 4, 30)\n",
    "rates = compute_fi_curve(currents)\n",
    "\n",
    "# Find rheobase (minimum current for spiking)\n",
    "rheobase_idx = np.where(rates > 0)[0]\n",
    "rheobase = currents[rheobase_idx[0]] if len(rheobase_idx) > 0 else None\n",
    "\n",
    "plot_fi_curve(currents, rates, rheobase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify solution with doctests\n",
    "doctest.run_docstring_examples(compute_fi_curve, globals(), name=\"compute_fi_curve\")\n",
    "\n",
    "# Compute and plot F-I curve\n",
    "currents = np.linspace(0, 4, 30)\n",
    "rates = compute_fi_curve(currents)\n",
    "\n",
    "# Find rheobase (minimum current for spiking)\n",
    "rheobase_idx = np.where(rates > 0)[0]\n",
    "rheobase = currents[rheobase_idx[0]] if len(rheobase_idx) > 0 else None\n",
    "\n",
    "plot_fi_curve(currents, rates, rheobase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Conductance-Based Synapses\n",
    "\n",
    "How do neurons receive input? Through **synapses**! When a presynaptic neuron fires, it releases neurotransmitter that opens ion channels in the postsynaptic neuron, creating a **conductance**.\n",
    "\n",
    "### The Synaptic Conductance Equation\n",
    "\n",
    "The conductance follows exponential decay after each spike:\n",
    "\n",
    "$$g_{syn}(t) = \\sum_{s} \\bar{g}_{max} \\cdot e^{-(t-t_s)/\\tau} \\cdot \\Theta(t-t_s)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\bar{g}_{max}$ is the maximum conductance per spike (nS)\n",
    "- $\\tau$ is the synaptic time constant (ms)\n",
    "- $t_s$ is the spike time\n",
    "- $\\Theta$ is the Heaviside step function (0 for negative, 1 for positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement Synaptic Conductance from the Equation\n",
    "\n",
    "**Your task**: Translate the conductance equation directly into code!\n",
    "\n",
    "$$g_{syn}(t) = \\sum_{s} \\bar{g}_{max} \\cdot e^{-(t-t_s)/\\tau} \\cdot \\Theta(t-t_s)$$\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- Only include spikes that have already occurred ($t \\geq t_s$)\n",
    "- The Heaviside function $\\Theta(x)$ returns 0 for $x < 0$ and 1 for $x \\geq 0$\n",
    "- Use `np.exp()` for the exponential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conductance_at_time(t: float, spike_times: list, g_max: float = 1.0, tau: float = 5.0) -> float:\n",
    "    \"\"\"Calculate total synaptic conductance at time t from a list of spike times.\n",
    "\n",
    "    Implements the equation:\n",
    "        g(t) = Î£ g_max * exp(-(t - t_s) / Ï„) * Î˜(t - t_s)\n",
    "\n",
    "    Where Î˜ is the Heaviside step function (0 for x<0, 1 for xâ‰¥0).\n",
    "\n",
    "    Args:\n",
    "        t: Current time (ms)\n",
    "        spike_times: List of presynaptic spike times (ms)\n",
    "        g_max: Maximum conductance per spike (nS)\n",
    "        tau: Decay time constant (ms)\n",
    "\n",
    "    Returns:\n",
    "        Total conductance at time t (nS)\n",
    "\n",
    "    Examples:\n",
    "        >>> conductance_at_time(10, [10], g_max=2.0, tau=5.0)  # At spike time\n",
    "        2.0\n",
    "        >>> round(conductance_at_time(15, [10], g_max=2.0, tau=5.0), 3)  # After 1 tau\n",
    "        0.736\n",
    "        >>> conductance_at_time(10, [20], g_max=2.0, tau=5.0)  # Before spike\n",
    "        0.0\n",
    "        >>> round(conductance_at_time(15, [10, 12], g_max=2.0, tau=5.0), 3)  # Two spikes sum\n",
    "        1.645\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # 1. Loop through each spike time\n",
    "    # 2. Only include spikes that have occurred\n",
    "    # 3. Sum up g for each valid spike\n",
    "    raise NotImplementedError(\"Implement conductance_at_time\")\n",
    "\n",
    "\n",
    "doctest.run_docstring_examples(conductance_at_time, globals(), name=\"conductance_at_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Synapse:\n",
    "    \"\"\"Conductance-based synapse model.\n",
    "\n",
    "    Models synaptic transmission with exponential conductance decay.\n",
    "\n",
    "    Attributes:\n",
    "        g_max: Maximum conductance (nS)\n",
    "        tau_syn: Synaptic time constant (ms)\n",
    "        e_syn: Reversal potential (mV)\n",
    "\n",
    "    Examples:\n",
    "        >>> syn = Synapse(g_max=2.0, tau_syn=5.0)\n",
    "        >>> syn.g  # Initially no conductance\n",
    "        0.0\n",
    "        >>> syn.receive_spike()\n",
    "        >>> syn.g  # After spike: g_max\n",
    "        2.0\n",
    "        >>> syn.step(dt=5.0)  # After one time constant\n",
    "        >>> round(syn.g, 3)  # Decayed to g_max * exp(-1)\n",
    "        0.736\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g_max: float = 1.0, tau_syn: float = 5.0, e_syn: float = 0.0):\n",
    "        self.g_max = g_max\n",
    "        self.tau_syn = tau_syn\n",
    "        self.e_syn = e_syn\n",
    "        self.g = 0.0\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset synapse to baseline.\"\"\"\n",
    "        self.g = 0.0\n",
    "\n",
    "    def receive_spike(self):\n",
    "        \"\"\"Process an incoming presynaptic spike.\"\"\"\n",
    "        self.g += self.g_max\n",
    "\n",
    "    def step(self, dt: float = 0.1) -> None:\n",
    "        \"\"\"Update conductance (exponential decay).\"\"\"\n",
    "        self.g *= np.exp(-dt / self.tau_syn)\n",
    "\n",
    "    def get_current(self, v_post: float) -> float:\n",
    "        \"\"\"Calculate synaptic current given postsynaptic voltage.\n",
    "\n",
    "        Args:\n",
    "            v_post: Postsynaptic membrane potential (mV)\n",
    "\n",
    "        Returns:\n",
    "            Synaptic current (nA)\n",
    "        \"\"\"\n",
    "        return self.g * (self.e_syn - v_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate synaptic conductance with multiple spikes\n",
    "dt = 0.1\n",
    "time = np.arange(0, 150, dt)\n",
    "spike_times_pre = [10, 30, 35, 40, 80, 120]  # Note the burst at 30-40 ms!\n",
    "\n",
    "synapse = Synapse(g_max=2.0, tau_syn=5.0)\n",
    "g_trace = []\n",
    "i_trace = []  # Track current too\n",
    "\n",
    "v_post = -70  # Assume constant postsynaptic voltage for now\n",
    "\n",
    "for t in time:\n",
    "    if any(abs(t - st) < dt / 2 for st in spike_times_pre):\n",
    "        synapse.receive_spike()\n",
    "    g_trace.append(synapse.g)\n",
    "    i_trace.append(synapse.get_current(v_post))\n",
    "    synapse.step(dt)\n",
    "\n",
    "# Plot\n",
    "plot_synaptic_response(time, g_trace, i_trace, spike_times_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Let's think about it!\n",
    "\n",
    "Notice the **temporal summation** at 30-40 ms! When spikes arrive close together:\n",
    "\n",
    "- The conductance **adds up** before it can decay\n",
    "- The response is **larger** than for isolated spikes\n",
    "- This is a form of **temporal integration** the perceptron cannot capture\n",
    "\n",
    "**Exploration**: What happens if you change `tau_syn`? Try 2 ms vs 20 ms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Short-Term Plasticity (Vesicle Depletion)\n",
    "\n",
    "Real synapses don't have unlimited neurotransmitter! Each spike releases vesicles from a **finite pool**, and the pool takes time to replenish.\n",
    "\n",
    "### The Tsodyks-Markram Model (Simplified)\n",
    "\n",
    "We track:\n",
    "\n",
    "- $x$: Fraction of available vesicles (0 to 1)\n",
    "- $u$: Release probability per spike\n",
    "\n",
    "On each spike:\n",
    "\n",
    "- Vesicles used: $\\Delta x = u \\cdot x$\n",
    "- Effective conductance: $g_{eff} = \\bar{g} \\cdot u \\cdot x$\n",
    "\n",
    "Between spikes, vesicles recover:\n",
    "$$\\frac{dx}{dt} = \\frac{1 - x}{\\tau_{rec}}$$\n",
    "\n",
    "This creates **synaptic depression**: repeated firing depletes vesicles!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepressingSynapse:\n",
    "    \"\"\"Synapse with short-term depression (vesicle depletion).\n",
    "\n",
    "    Implements a simplified Tsodyks-Markram model where repeated\n",
    "    activation depletes the vesicle pool, reducing synaptic strength.\n",
    "\n",
    "    Attributes:\n",
    "        g_max: Maximum conductance when fully recovered (nS)\n",
    "        tau_syn: Conductance decay time constant (ms)\n",
    "        tau_rec: Vesicle recovery time constant (ms)\n",
    "        u: Release probability per spike\n",
    "        e_syn: Reversal potential (mV)\n",
    "\n",
    "    Examples:\n",
    "        >>> syn = DepressingSynapse(g_max=4.0, u=0.5)\n",
    "        >>> syn.x  # Full vesicle pool\n",
    "        1.0\n",
    "        >>> syn.receive_spike()\n",
    "        >>> syn.g  # First spike: g_max * u * x = 4 * 0.5 * 1 = 2\n",
    "        2.0\n",
    "        >>> syn.x  # Pool depleted: 1 - 0.5*1 = 0.5\n",
    "        0.5\n",
    "        >>> syn.receive_spike()  # Second spike immediately\n",
    "        >>> syn.g  # Weaker: previous g + 4 * 0.5 * 0.5 = 2 + 1 = 3\n",
    "        3.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        g_max: float = 1.0,\n",
    "        tau_syn: float = 5.0,\n",
    "        tau_rec: float = 200.0,\n",
    "        u: float = 0.5,\n",
    "        e_syn: float = 0.0,\n",
    "    ):\n",
    "        self.g_max = g_max\n",
    "        self.tau_syn = tau_syn\n",
    "        self.tau_rec = tau_rec\n",
    "        self.u = u\n",
    "        self.e_syn = e_syn\n",
    "        self.g = 0.0\n",
    "        self.x = 1.0  # Full pool\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset to fully recovered state.\"\"\"\n",
    "        self.g = 0.0\n",
    "        self.x = 1.0\n",
    "\n",
    "    def receive_spike(self):\n",
    "        \"\"\"Process incoming spike with vesicle depletion.\"\"\"\n",
    "        released = self.u * self.x\n",
    "        self.x -= released\n",
    "        self.g += self.g_max * released\n",
    "\n",
    "    def step(self, dt: float = 0.1) -> None:\n",
    "        \"\"\"Update conductance decay and vesicle recovery.\"\"\"\n",
    "        self.g *= np.exp(-dt / self.tau_syn)\n",
    "        self.x += (1 - self.x) * (dt / self.tau_rec)\n",
    "\n",
    "    def get_current(self, v_post: float) -> float:\n",
    "        \"\"\"Calculate synaptic current.\"\"\"\n",
    "        return self.g * (self.e_syn - v_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regular vs depressing synapse\n",
    "dt = 0.1\n",
    "time = np.arange(0, 500, dt)\n",
    "\n",
    "# Regular spike train at 20 Hz\n",
    "spike_times_pre = np.arange(50, 400, 50)\n",
    "\n",
    "# Regular synapse\n",
    "syn_regular = Synapse(g_max=2.0, tau_syn=5.0)\n",
    "g_regular = []\n",
    "\n",
    "# Depressing synapse\n",
    "syn_depressing = DepressingSynapse(g_max=4.0, tau_syn=5.0, tau_rec=200.0, u=0.5)\n",
    "g_depressing = []\n",
    "x_trace = []\n",
    "\n",
    "for t in time:\n",
    "    is_spike = any(abs(t - st) < dt / 2 for st in spike_times_pre)\n",
    "\n",
    "    if is_spike:\n",
    "        syn_regular.receive_spike()\n",
    "        syn_depressing.receive_spike()\n",
    "\n",
    "    g_regular.append(syn_regular.g)\n",
    "    g_depressing.append(syn_depressing.g)\n",
    "    x_trace.append(syn_depressing.x)\n",
    "\n",
    "    syn_regular.step(dt)\n",
    "    syn_depressing.step(dt)\n",
    "\n",
    "# Plot comparison\n",
    "plot_synapse_comparison(\n",
    "    time, np.array(g_regular), np.array(g_depressing), np.array(x_trace), spike_times_pre\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’­ Let's think about it!\n",
    "\n",
    "Compare the two synapse types:\n",
    "\n",
    "1. **Regular synapse**: Each spike â†’ same response. **Stateless**.\n",
    "2. **Depressing synapse**: First spike â†’ strong. Later spikes â†’ weaker. Has **memory**!\n",
    "\n",
    "**Why is this computationally useful?**\n",
    "\n",
    "- **High-pass filter**: Responds strongly to _onset_ of activity\n",
    "- **Gain control**: Prevents runaway excitation\n",
    "- **Temporal differentiation**: Sensitive to _changes_ in input rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Explore STP Parameters\n",
    "\n",
    "Investigate how different short-term plasticity parameters affect synaptic transmission.\n",
    "\n",
    "**Try:**\n",
    "\n",
    "1. Fast recovery (`tau_rec=50`): Does depression still occur?\n",
    "2. High release probability (`u=0.8`): How does the first vs later responses compare?\n",
    "3. Low release probability (`u=0.2`): What changes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exploration here!\n",
    "# Create synapses with different parameters and compare their responses\n",
    "\n",
    "# Example setup:\n",
    "params = [\n",
    "    {\"name\": \"Standard\", \"tau_rec\": 200, \"u\": 0.5, \"color\": \"blue\"},\n",
    "    {\"name\": \"Fast Recovery\", \"tau_rec\": 50, \"u\": 0.5, \"color\": \"green\"},\n",
    "    {\"name\": \"High Release\", \"tau_rec\": 200, \"u\": 0.8, \"color\": \"red\"},\n",
    "]\n",
    "\n",
    "# TODO: For each parameter set:\n",
    "# 1. Create a DepressingSynapse with those parameters\n",
    "# 2. Simulate responses to a spike train\n",
    "# 3. Plot and compare the results\n",
    "plot_stp_comparison(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement a Facilitating Synapse\n",
    "\n",
    "So far we've seen **depressing** synapses that get weaker with repeated use. But some synapses show the opposite behavior: **short-term facilitation** where the release probability _increases_ after each spike!\n",
    "\n",
    "**The Mechanism:**\n",
    "\n",
    "- Each spike temporarily increases the release probability $u$\n",
    "- $u$ decays back to baseline $u_{base}$ over time\n",
    "- This creates responses that get _stronger_ with repeated activation\n",
    "\n",
    "**Your task**: Implement a `FacilitatingSynapse` class with the following dynamics:\n",
    "\n",
    "- On each spike: $u \\leftarrow u + U_{step} \\cdot (u_{max} - u)$, then release conductance $g_{max} \\cdot u$\n",
    "- Between spikes: $u$ decays toward $u_{base}$ with time constant $\\tau_{facil}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacilitatingSynapse:\n",
    "    \"\"\"Synapse with short-term facilitation.\n",
    "\n",
    "    Unlike depressing synapses, facilitating synapses get STRONGER\n",
    "    with repeated use. The release probability u increases after\n",
    "    each spike and decays back to baseline between spikes.\n",
    "\n",
    "    Attributes:\n",
    "        g_max: Maximum conductance (nS)\n",
    "        tau_syn: Conductance decay time constant (ms)\n",
    "        tau_facil: Facilitation decay time constant (ms)\n",
    "        u_base: Baseline release probability\n",
    "        u_max: Maximum release probability\n",
    "        u_step: Facilitation increment per spike\n",
    "        e_syn: Reversal potential (mV)\n",
    "\n",
    "    Examples:\n",
    "        >>> syn = FacilitatingSynapse(g_max=4.0, u_base=0.1, u_max=0.8, u_step=0.3)\n",
    "        >>> syn.u  # Starts at baseline\n",
    "        0.1\n",
    "        >>> syn.receive_spike()\n",
    "        >>> round(syn.g, 2)  # First spike: weak (g_max * 0.1 = 0.4)\n",
    "        0.4\n",
    "        >>> round(syn.u, 2)  # u increased toward u_max\n",
    "        0.31\n",
    "        >>> syn.receive_spike()  # Second spike immediately\n",
    "        >>> round(syn.g, 2)  # Stronger! (0.4 + 4 * 0.31 = 1.64)\n",
    "        1.64\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        g_max: float = 1.0,\n",
    "        tau_syn: float = 5.0,\n",
    "        tau_facil: float = 100.0,\n",
    "        u_base: float = 0.1,\n",
    "        u_max: float = 0.8,\n",
    "        u_step: float = 0.3,\n",
    "        e_syn: float = 0.0,\n",
    "    ):\n",
    "        # TODO: Initialize all attributes\n",
    "        # - g_max, tau_syn, tau_facil, u_base, u_max, u_step, e_syn\n",
    "        # - g: current conductance (starts at 0)\n",
    "        # - u: current release probability (starts at u_base)\n",
    "        raise NotImplementedError(\"Implement __init__\")\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset to baseline state.\"\"\"\n",
    "        # TODO: Reset g to 0 and u to u_base\n",
    "        raise NotImplementedError(\"Implement reset_state\")\n",
    "\n",
    "    def receive_spike(self):\n",
    "        \"\"\"Process incoming spike with facilitation.\n",
    "\n",
    "        1. Add conductance\n",
    "        2. Increase u toward u_max\n",
    "        \"\"\"\n",
    "        # TODO: Implement facilitation dynamics\n",
    "        raise NotImplementedError(\"Implement receive_spike\")\n",
    "\n",
    "    def step(self, dt: float = 0.1) -> None:\n",
    "        \"\"\"Update conductance decay and u decay back to baseline.\n",
    "\n",
    "        - g decays exponentially with tau_syn\n",
    "        - u decays toward u_base with tau_facil\n",
    "        \"\"\"\n",
    "        # TODO: Implement decay dynamics\n",
    "        raise NotImplementedError(\"Implement step\")\n",
    "\n",
    "    def get_current(self, v_post: float) -> float:\n",
    "        \"\"\"Calculate synaptic current.\"\"\"\n",
    "        return self.g * (self.e_syn - v_post)\n",
    "\n",
    "\n",
    "doctest.run_docstring_examples(FacilitatingSynapse.__doc__, globals(), name=\"FacilitatingSynapse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare depressing vs facilitating synapses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dt = 0.1\n",
    "time = np.arange(0, 500, dt)\n",
    "spike_times_pre = np.arange(50, 400, 50)\n",
    "\n",
    "# Depressing synapse\n",
    "syn_dep = DepressingSynapse(g_max=4.0, tau_syn=5.0, tau_rec=200.0, u=0.5)\n",
    "g_dep = []\n",
    "\n",
    "# Facilitating synapse\n",
    "syn_fac = FacilitatingSynapse(\n",
    "    g_max=4.0, tau_syn=5.0, tau_facil=100.0, u_base=0.1, u_max=0.8, u_step=0.3\n",
    ")\n",
    "g_fac = []\n",
    "u_trace = []\n",
    "\n",
    "for t in time:\n",
    "    is_spike = any(abs(t - st) < dt / 2 for st in spike_times_pre)\n",
    "\n",
    "    if is_spike:\n",
    "        syn_dep.receive_spike()\n",
    "        syn_fac.receive_spike()\n",
    "\n",
    "    g_dep.append(syn_dep.g)\n",
    "    g_fac.append(syn_fac.g)\n",
    "    u_trace.append(syn_fac.u)\n",
    "\n",
    "    syn_dep.step(dt)\n",
    "    syn_fac.step(dt)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "axes[0].plot(time, g_dep, \"b-\", label=\"Depressing\", linewidth=1.5)\n",
    "axes[0].plot(time, g_fac, \"r-\", label=\"Facilitating\", linewidth=1.5)\n",
    "for st in spike_times_pre:\n",
    "    axes[0].axvline(st, color=\"gray\", alpha=0.3, linestyle=\"--\")\n",
    "axes[0].set_ylabel(\"Conductance (nS)\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"Depression vs Facilitation: Opposite Temporal Filters\")\n",
    "\n",
    "axes[1].plot(time, u_trace, \"r-\", label=\"u (facilitating)\", linewidth=1.5)\n",
    "axes[1].axhline(syn_fac.u_base, color=\"gray\", linestyle=\":\", label=f\"u_base={syn_fac.u_base}\")\n",
    "for st in spike_times_pre:\n",
    "    axes[1].axvline(st, color=\"gray\", alpha=0.3, linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"Time (ms)\")\n",
    "axes[1].set_ylabel(\"Release probability u\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Depressing synapse: First spike is STRONGEST (high-pass filter)\")\n",
    "print(\"Facilitating synapse: Later spikes are STRONGER (low-pass filter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Putting It All Together\n",
    "\n",
    "Now let's connect a depressing synapse to an LIF neuron - the complete biological neuron model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFWithSynapse:\n",
    "    \"\"\"LIF neuron with conductance-based synaptic input.\n",
    "\n",
    "    Combines LIF membrane dynamics with a depressing synapse.\n",
    "\n",
    "    Examples:\n",
    "        >>> neuron = LIFWithSynapse(g_max=5.0)\n",
    "        >>> neuron.v == neuron.v_rest\n",
    "        True\n",
    "        >>> neuron.synapse.x  # Full vesicle pool\n",
    "        1.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau_m: float = 20.0,\n",
    "        v_rest: float = -70.0,\n",
    "        v_reset: float = -80.0,\n",
    "        v_threshold: float = -55.0,\n",
    "        g_max: float = 5.0,\n",
    "        tau_syn: float = 5.0,\n",
    "        tau_rec: float = 200.0,\n",
    "        u: float = 0.5,\n",
    "        e_syn: float = 0.0,\n",
    "    ):\n",
    "        self.tau_m = tau_m\n",
    "        self.v_rest = v_rest\n",
    "        self.v_reset = v_reset\n",
    "        self.v_threshold = v_threshold\n",
    "        self.e_syn = e_syn\n",
    "        self.synapse = DepressingSynapse(g_max, tau_syn, tau_rec, u, e_syn)\n",
    "        self.v = v_rest\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.v = self.v_rest\n",
    "        self.synapse.reset_state()\n",
    "\n",
    "    def step(self, presynaptic_spike: bool, dt: float = 0.1) -> bool:\n",
    "        \"\"\"Simulate one time step.\"\"\"\n",
    "        if presynaptic_spike:\n",
    "            self.synapse.receive_spike()\n",
    "\n",
    "        I_syn = self.synapse.get_current(self.v)\n",
    "        dv = (-(self.v - self.v_rest) + I_syn) * (dt / self.tau_m)\n",
    "        self.v += dv\n",
    "        self.synapse.step(dt)\n",
    "\n",
    "        if self.v >= self.v_threshold:\n",
    "            self.v = self.v_reset\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def simulate(self, presynaptic_spikes: np.ndarray, dt: float = 0.1):\n",
    "        \"\"\"Simulate given presynaptic spike train.\"\"\"\n",
    "        self.reset_state()\n",
    "        v_trace, g_trace, x_trace = [], [], []\n",
    "        spike_times = []\n",
    "\n",
    "        for t_idx, is_spike in enumerate(presynaptic_spikes):\n",
    "            spiked = self.step(is_spike, dt)\n",
    "            v_trace.append(self.v)\n",
    "            g_trace.append(self.synapse.g)\n",
    "            x_trace.append(self.synapse.x)\n",
    "            if spiked:\n",
    "                spike_times.append(t_idx * dt)\n",
    "\n",
    "        return np.array(v_trace), np.array(g_trace), np.array(x_trace), spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate with Poisson input\n",
    "dt = 0.1\n",
    "duration = 1000\n",
    "time = np.arange(0, duration, dt)\n",
    "\n",
    "# Poisson spike train (~30 Hz)\n",
    "rate = 30\n",
    "p_spike = rate * dt / 1000\n",
    "presynaptic_spikes = rng.random(len(time)) < p_spike\n",
    "\n",
    "# Simulate\n",
    "neuron = LIFWithSynapse(g_max=8.0, tau_rec=150.0, u=0.4)\n",
    "v_trace, g_trace, x_trace, spike_times = neuron.simulate(presynaptic_spikes, dt)\n",
    "\n",
    "print(\n",
    "    f\"Presynaptic spikes: {presynaptic_spikes.sum()} ({presynaptic_spikes.sum() / (duration / 1000):.1f} Hz)\"\n",
    ")\n",
    "print(f\"Postsynaptic spikes: {len(spike_times)} ({len(spike_times) / (duration / 1000):.1f} Hz)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "plot_complete_neuron(\n",
    "    time, presynaptic_spikes, g_trace, x_trace, v_trace, spike_times, neuron.v_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Side-by-Side Comparison\n",
    "\n",
    "Let's compare how the perceptron and biological neuron respond to varying input intensities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_io_curves(intensities: np.ndarray, duration: float = 500, dt: float = 0.1):\n",
    "    \"\"\"Compare I-O curves for perceptron and LIF.\"\"\"\n",
    "    # Perceptron\n",
    "    p = Perceptron(n_inputs=1)\n",
    "    p.weights = np.array([1.0])\n",
    "    p.bias = 0.0\n",
    "    p_outputs = [p.forward(np.array([i])) for i in intensities]\n",
    "\n",
    "    # LIF with synapse\n",
    "    lif = LIFWithSynapse(g_max=10.0, tau_rec=150.0, u=0.4)\n",
    "    lif_rates = []\n",
    "\n",
    "    for intensity in intensities:\n",
    "        rate = max(0, intensity * 50)\n",
    "        time = np.arange(0, duration, dt)\n",
    "        spikes = rng.random(len(time)) < (rate * dt / 1000)\n",
    "        _, _, _, spike_times = lif.simulate(spikes, dt)\n",
    "        lif_rates.append(len(spike_times) / (duration / 1000))\n",
    "\n",
    "    return np.array(p_outputs), np.array(lif_rates)\n",
    "\n",
    "\n",
    "intensities = np.linspace(0, 3, 25)\n",
    "p_outputs, lif_rates = compare_io_curves(intensities)\n",
    "\n",
    "# Plot comparison\n",
    "plot_io_comparison(intensities, p_outputs, intensities * 50, lif_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you implemented:\n",
    "\n",
    "1. **The Perceptron** - Instantaneous, stateless weighted sum + activation\n",
    "2. **The LIF Neuron** - Dynamic membrane potential with temporal integration\n",
    "3. **Conductance-Based Synapses** - Realistic input: spikes â†’ conductance â†’ current\n",
    "4. **Short-Term Plasticity** - Vesicle depletion creates history-dependent transmission\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect         | Perceptron           | Biological Neuron           |\n",
    "| -------------- | -------------------- | --------------------------- |\n",
    "| **Input**      | Static vector        | Spike train â†’ conductance   |\n",
    "| **Processing** | Instant weighted sum | Temporal integration + leak |\n",
    "| **Output**     | Continuous value     | Discrete spike train        |\n",
    "| **Memory**     | None                 | Membrane state, vesicles    |\n",
    "| **Plasticity** | Fixed weights        | Short-term depression       |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Time matters**: Biological neurons process information over time\n",
    "- **History matters**: Short-term plasticity = context-dependent processing\n",
    "- **Spikes are sparse**: Discrete events, not continuous rates\n",
    "- **Neither is \"better\"**: Each captures different computational principles\n",
    "\n",
    "### References\n",
    "\n",
    "1. Gerstner, W., et al. (2014). _Neuronal Dynamics_. Cambridge University Press.\n",
    "2. Tsodyks, M., & Markram, H. (1997). The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability. _PNAS_, 94(2), 719-723.\n",
    "3. Abbott, L. F., & Regehr, W. G. (2004). Synaptic computation. _Nature_, 431(7010), 796-803.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroai-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
